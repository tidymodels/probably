[{"path":[]},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://probably.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://probably.tidymodels.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 probably authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://probably.tidymodels.org/dev/articles/equivocal-zones.html","id":"equivocal-zones","dir":"Articles","previous_headings":"","what":"Equivocal zones","title":"Equivocal zones","text":"fields, class probability predictions must meet certain standards firm decision can made using . fail standards, prediction can marked equivocal, just means unsure true result. might want investigate equivocal values, rerun whatever process generated proceeding. example, binary model, prediction returned probability values 52% Yes 48% , really sure isn’t just random noise? case, use buffer surrounding threshold 50% determine whether model sure predictions, mark values unsure equivocal. Another example come Bayesian perspective, prediction comes probability distribution. model might predict 80% Yes, standard deviation around +/- 20%. case, set maximum allowed standard deviation cutoff whether mark values equivocal. work equivocal zones, probably provides new class hard class predictions similar factor, allows mark certain values equivocal. reportable rate fraction values equivocal, relative total number. , can see reportable rate started 100%, soon single value marked equivocal, value dropped 75%. fields equivocal zones used, often tradeoff marking values equivocal keeping certain minimum reportable rate. Generally, won’t create class_pred objects directly, instead create indirectly converting class probabilities class predictions make_class_pred() make_two_class_pred(). buffer used, equivocal zone created around threshold threshold +/- buffer values inside zone automatically marked equivocal. Equivocal values class_pred objects converted NA object converted factor. ’s also worth noting [EQ] label treated separate level. NA behavior feeds probably can used yardstick. Generally, equivocal values removed completely performance evaluation. converting NA leaving default na_rm = TRUE yardstick metric removes consideration. seen , removing equivocal values using simple threshold generally improves performance values model unsure removed. don’t fooled! give cases extra consideration, remember reportable rate decreased removing . production, ’ll likely something predictions!","code":"x <- factor(c(\"Yes\", \"No\", \"Yes\", \"Yes\"))  # Create a class_pred object from a factor class_pred(x) #> [1] Yes No  Yes Yes #> Levels: No Yes #> Reportable: 100%  # Say you aren't sure about that 2nd \"Yes\" value.  # You could mark it as equivocal. class_pred(x, which = 3) #> [1] Yes  No   [EQ] Yes  #> Levels: No Yes #> Reportable: 75% library(dplyr) data(\"segment_logistic\") segment_logistic #> # A tibble: 1,010 × 3 #>    .pred_poor .pred_good Class #>  *      <dbl>      <dbl> <fct> #>  1    0.986      0.0142  poor  #>  2    0.897      0.103   poor  #>  3    0.118      0.882   good  #>  4    0.102      0.898   good  #>  5    0.991      0.00914 poor  #>  6    0.633      0.367   good  #>  7    0.770      0.230   good  #>  8    0.00842    0.992   good  #>  9    0.995      0.00458 poor  #> 10    0.765      0.235   poor  #> # ℹ 1,000 more rows  # Convert probabilities into predictions # > 0.5 = good # < 0.5 = poor segment_logistic_thresh <- segment_logistic %>%   mutate(     .pred = make_two_class_pred(       estimate = .pred_good,        levels = levels(Class),        threshold = 0.5     )   )  segment_logistic_thresh #> # A tibble: 1,010 × 4 #>    .pred_poor .pred_good Class      .pred #>         <dbl>      <dbl> <fct> <clss_prd> #>  1    0.986      0.0142  poor        poor #>  2    0.897      0.103   poor        poor #>  3    0.118      0.882   good        good #>  4    0.102      0.898   good        good #>  5    0.991      0.00914 poor        poor #>  6    0.633      0.367   good        poor #>  7    0.770      0.230   good        poor #>  8    0.00842    0.992   good        good #>  9    0.995      0.00458 poor        poor #> 10    0.765      0.235   poor        poor #> # ℹ 1,000 more rows # Convert probabilities into predictions #        x > 0.55 = good #        x < 0.45 = poor # 0.45 < x < 0.55 = equivocal segment_pred <- segment_logistic %>%   mutate(     .pred = make_two_class_pred(       estimate = .pred_good,        levels = levels(Class),        threshold = 0.5,       buffer = 0.05     )   )   segment_pred %>%   count(.pred) #> # A tibble: 3 × 2 #>        .pred     n #>   <clss_prd> <int> #> 1       [EQ]    45 #> 2       good   340 #> 3       poor   625  segment_pred %>%   summarise(reportable = reportable_rate(.pred)) #> # A tibble: 1 × 1 #>   reportable #>        <dbl> #> 1      0.955 segment_pred %>%   mutate(.pred_fct = as.factor(.pred)) %>%   count(.pred, .pred_fct) #> # A tibble: 3 × 3 #>        .pred .pred_fct     n #>   <clss_prd> <fct>     <int> #> 1       [EQ] NA           45 #> 2       good good        340 #> 3       poor poor        625  levels(segment_pred$.pred) #> [1] \"good\" \"poor\" library(yardstick)  # No equivocal zone segment_logistic_thresh %>%   mutate(.pred_fct = as.factor(.pred)) %>%   precision(Class, .pred_fct) #> # A tibble: 1 × 3 #>   .metric   .estimator .estimate #>   <chr>     <chr>          <dbl> #> 1 precision binary         0.680  # Equivocal zone segment_pred %>%   mutate(.pred_fct = as.factor(.pred)) %>%   precision(Class, .pred_fct) #> # A tibble: 1 × 3 #>   .metric   .estimator .estimate #>   <chr>     <chr>          <dbl> #> 1 precision binary         0.694"},{"path":"https://probably.tidymodels.org/dev/articles/where-to-use.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Where does probably fit in?","text":"obvious question regarding probably might : fit rest tidymodels ecosystem? Like pieces ecosystem, probably designed modular, plays well tidymodels packages. Regarding placement modeling workflow, probably best fits post processing step model fit, model performance calculated.","code":""},{"path":"https://probably.tidymodels.org/dev/articles/where-to-use.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Where does probably fit in?","text":"example, ’ll use parsnip fit logistic regression Lending Club loan data, use probably investigate happens performance vary threshold “good” loan . Let’s split 75% training 25% testing something predict . anything, let’s look counts going predicting, Class loan. Clearly large imbalance number good bad loans. probably good thing bank, poses interesting issue us might want ensure sensitive bad loans overwhelmed number good ones. One thing might downsample number good loans total number line number bad loans. fitting model using themis::step_downsample(), now, let’s continue data unchanged. ’ll use parsnip’s logistic_reg() create model specification logistic regression, set engine glm actually fit model using data model formula. output parsnip fit() call parsnip model_fit object, underlying print method glm fit used. Now let’s predict testing set, use type = \"prob\" get class probabilities back rather hard predictions. use probably investigate performance. class probabilities hand, can use make_two_class_pred() convert probabilities hard predictions using threshold. threshold 0.5 just says predicted probability 0.5, classify prediction “good” loan, otherwise, bad. Hmm, 0.5 threshold, almost loans predicted “good”. Perhaps something large class imbalance. hand, bank might want stringent classified “good” loan, might require probability 0.75 threshold. case, 4 bad loans correctly classified bad, good loans also misclassified bad now. tradeoff , can somewhat captured metrics sensitivity specificity. metrics max value 1. sensitivity - proportion predicted “good” loans “good” loans specificity - proportion predicted “bad” loans “bad” loans example, increased specificity (capturing 4 bad loans higher threshold), lowered sensitivity (incorrectly reclassifying good loans bad). nice combination metrics represent tradeoff. Luckily, j_index exactly . \\[ j\\_index = sens + spec - 1 \\] j_index maximum value 1 false positives false negatives. can used justification whether increase threshold value worth . increasing threshold results increase specificity decrease sensitivity, can see j_index. Now, way optimize things. care low false positives, might interested keeping sensitivity high, wouldn’t best way tackle problem. now, let’s see can use probably optimize j_index. threshold_perf() recalculate number metrics across varying thresholds. One j_index. ggplot2, can easily visualize varying performance find optimal threshold maximizing j_index.  ’s clear visual optimal threshold high, exactly 0.945. pretty high, , optimization method won’t useful cases. wrap , test set metrics threshold value.","code":"library(parsnip) library(probably) library(dplyr) library(rsample) library(modeldata) data(\"lending_club\")  # I think it makes more sense to have \"good\" as the first level # By default it comes as the second level lending_club <- lending_club %>%   mutate(Class = relevel(Class, \"good\"))  # There are a number of columns in this data set, but we will only use a few # for this example lending_club <- select(lending_club, Class, annual_inc, verification_status, sub_grade)  lending_club #> # A tibble: 9,857 × 4 #>    Class annual_inc verification_status sub_grade #>    <fct>      <dbl> <fct>               <fct>     #>  1 good       35000 Not_Verified        C4        #>  2 good       72000 Verified            C1        #>  3 good       72000 Source_Verified     D1        #>  4 good      101000 Verified            C3        #>  5 good       50100 Source_Verified     A4        #>  6 good       32000 Source_Verified     B5        #>  7 good       65000 Not_Verified        A1        #>  8 good      188000 Not_Verified        B2        #>  9 good       89000 Source_Verified     B3        #> 10 good       48000 Not_Verified        C2        #> # ℹ 9,847 more rows # 75% train, 25% test set.seed(123)  split <- initial_split(lending_club, prop = 0.75)  lending_train <- training(split) lending_test  <- testing(split) count(lending_train, Class) #> # A tibble: 2 × 2 #>   Class     n #>   <fct> <int> #> 1 good   7008 #> 2 bad     384 logi_reg <- logistic_reg() logi_reg_glm <- logi_reg %>% set_engine(\"glm\")  # A small model specification that defines the type of model you are  # using and the engine logi_reg_glm #> Logistic Regression Model Specification (classification) #>  #> Computational engine: glm  # Fit the model logi_reg_fit <- fit(   logi_reg_glm,    formula = Class ~ annual_inc + verification_status + sub_grade,    data = lending_train )  logi_reg_fit #> parsnip model object #>  #>  #> Call:  stats::glm(formula = Class ~ annual_inc + verification_status +  #>     sub_grade, family = stats::binomial, data = data) #>  #> Coefficients: #>                        (Intercept)                          annual_inc   #>                         -5.670e+00                           1.915e-06   #> verification_statusSource_Verified         verification_statusVerified   #>                          4.324e-02                           3.364e-01   #>                        sub_gradeA2                         sub_gradeA3   #>                          9.508e-02                           1.149e+00   #>                        sub_gradeA4                         sub_gradeA5   #>                         -5.591e-02                           1.510e+00   #>                        sub_gradeB1                         sub_gradeB2   #>                          1.637e+00                           1.177e+00   #>                        sub_gradeB3                         sub_gradeB4   #>                          1.467e+00                           1.975e+00   #>                        sub_gradeB5                         sub_gradeC1   #>                          2.125e+00                           2.234e+00   #>                        sub_gradeC2                         sub_gradeC3   #>                          2.176e+00                           2.380e+00   #>                        sub_gradeC4                         sub_gradeC5   #>                          2.724e+00                           3.084e+00   #>                        sub_gradeD1                         sub_gradeD2   #>                          3.105e+00                           2.816e+00   #>                        sub_gradeD3                         sub_gradeD4   #>                          3.165e+00                           3.125e+00   #>                        sub_gradeD5                         sub_gradeE1   #>                          3.507e+00                           3.621e+00   #>                        sub_gradeE2                         sub_gradeE3   #>                          3.272e+00                           3.542e+00   #>                        sub_gradeE4                         sub_gradeE5   #>                          3.428e+00                           3.468e+00   #>                        sub_gradeF1                         sub_gradeF2   #>                          3.717e+00                           4.096e+00   #>                        sub_gradeF3                         sub_gradeF4   #>                          3.681e+00                           3.662e+00   #>                        sub_gradeF5                         sub_gradeG1   #>                          3.586e+00                           4.168e+00   #>                        sub_gradeG2                         sub_gradeG3   #>                          4.162e+00                           4.422e+00   #>                        sub_gradeG4                         sub_gradeG5   #>                          5.102e+00                          -8.226e+00   #>  #> Degrees of Freedom: 7391 Total (i.e. Null);  7354 Residual #> Null Deviance:       3019  #> Residual Deviance: 2716  AIC: 2792 predictions <- logi_reg_fit %>%   predict(new_data = lending_test, type = \"prob\")  head(predictions, n = 2) #> # A tibble: 2 × 2 #>   .pred_good .pred_bad #>        <dbl>     <dbl> #> 1      0.969    0.0311 #> 2      0.965    0.0353  lending_test_pred <- bind_cols(predictions, lending_test)  lending_test_pred #> # A tibble: 2,465 × 6 #>    .pred_good .pred_bad Class annual_inc verification_status sub_grade #>         <dbl>     <dbl> <fct>      <dbl> <fct>               <fct>     #>  1      0.969   0.0311  good       32000 Source_Verified     B5        #>  2      0.965   0.0353  good       73400 Source_Verified     C2        #>  3      0.960   0.0405  good      175000 Source_Verified     B5        #>  4      0.972   0.0276  good       70000 Not_Verified        B4        #>  5      0.874   0.126   good       36000 Source_Verified     E1        #>  6      0.944   0.0560  good       40000 Source_Verified     C4        #>  7      0.996   0.00385 good       60000 Not_Verified        A1        #>  8      0.951   0.0486  good       65000 Verified            C1        #>  9      0.963   0.0370  good       52000 Verified            B4        #> 10      0.983   0.0173  good       61000 Verified            B2        #> # ℹ 2,455 more rows hard_pred_0.5 <- lending_test_pred %>%   mutate(     .pred = make_two_class_pred(       estimate = .pred_good,        levels = levels(Class),        threshold = .5     )   ) %>%   select(Class, contains(\".pred\"))  hard_pred_0.5 %>%    count(.truth = Class, .pred) #> # A tibble: 2 × 3 #>   .truth      .pred     n #>   <fct>  <clss_prd> <int> #> 1 good         good  2332 #> 2 bad          good   133 hard_pred_0.75 <- lending_test_pred %>%   mutate(     .pred = make_two_class_pred(       estimate = .pred_good,        levels = levels(Class),        threshold = .75     )   ) %>%   select(Class, contains(\".pred\"))  hard_pred_0.75 %>%    count(.truth = Class, .pred) #> # A tibble: 4 × 3 #>   .truth      .pred     n #>   <fct>  <clss_prd> <int> #> 1 good         good  2320 #> 2 good          bad    12 #> 3 bad          good   129 #> 4 bad           bad     4 library(yardstick)  sens(hard_pred_0.5, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 sens    binary             1 spec(hard_pred_0.5, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 spec    binary             0  sens(hard_pred_0.75, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 sens    binary         0.995 spec(hard_pred_0.75, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 spec    binary        0.0301 j_index(hard_pred_0.5, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 j_index binary             0 j_index(hard_pred_0.75, Class, .pred) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 j_index binary        0.0249 threshold_data <- lending_test_pred %>%   threshold_perf(Class, .pred_good, thresholds = seq(0.5, 1, by = 0.0025))  threshold_data %>%   filter(.threshold %in% c(0.5, 0.6, 0.7)) #> # A tibble: 9 × 4 #>   .threshold .metric     .estimator .estimate #>        <dbl> <chr>       <chr>          <dbl> #> 1        0.5 sensitivity binary        1      #> 2        0.6 sensitivity binary        0.999  #> 3        0.7 sensitivity binary        0.998  #> 4        0.5 specificity binary        0      #> 5        0.6 specificity binary        0.0226 #> 6        0.7 specificity binary        0.0226 #> 7        0.5 j_index     binary        0      #> 8        0.6 j_index     binary        0.0217 #> 9        0.7 j_index     binary        0.0208 library(ggplot2)  threshold_data <- threshold_data %>%   filter(.metric != \"distance\") %>%   mutate(group = case_when(     .metric == \"sens\" | .metric == \"spec\" ~ \"1\",     TRUE ~ \"2\"   ))  max_j_index_threshold <- threshold_data %>%   filter(.metric == \"j_index\") %>%   filter(.estimate == max(.estimate)) %>%   pull(.threshold)  ggplot(threshold_data, aes(x = .threshold, y = .estimate, color = .metric, alpha = group)) +   geom_line() +   theme_minimal() +   scale_color_viridis_d(end = 0.9) +   scale_alpha_manual(values = c(.4, 1), guide = \"none\") +   geom_vline(xintercept = max_j_index_threshold, alpha = .6, color = \"grey30\") +   labs(     x = \"'Good' Threshold\\n(above this value is considered 'good')\",     y = \"Metric Estimate\",     title = \"Balancing performance by varying the threshold\",     subtitle = \"Sensitivity or specificity alone might not be enough!\\nVertical line = Max J-Index\"   ) threshold_data %>%   filter(.threshold == max_j_index_threshold) #> # A tibble: 3 × 5 #>   .threshold .metric     .estimator .estimate group #>        <dbl> <chr>       <chr>          <dbl> <chr> #> 1      0.945 sensitivity binary         0.687 2     #> 2      0.945 specificity binary         0.692 2     #> 3      0.945 j_index     binary         0.379 2"},{"path":"https://probably.tidymodels.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Max Kuhn. Author, maintainer. Davis Vaughan. Author. Edgar Ruiz. Author. . Copyright holder, funder.","code":""},{"path":"https://probably.tidymodels.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kuhn M, Vaughan D, Ruiz E (2023). probably: Tools Post-Processing Class Probability Estimates. https://github.com/tidymodels/probably/, https://probably.tidymodels.org.","code":"@Manual{,   title = {probably: Tools for Post-Processing Class Probability Estimates},   author = {Max Kuhn and Davis Vaughan and Edgar Ruiz},   year = {2023},   note = {https://github.com/tidymodels/probably/, https://probably.tidymodels.org}, }"},{"path":[]},{"path":"https://probably.tidymodels.org/dev/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Tools for Post-Processing Class Probability Estimates","text":"probably contains tools facilitate activities : Conversion probabilities discrete class predictions. Investigating estimating optimal probability thresholds. Calibration assessments remediation classification regression models. Inclusion equivocal zones probabilities uncertain report prediction.","code":""},{"path":"https://probably.tidymodels.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools for Post-Processing Class Probability Estimates","text":"can install probably CRAN : can install development version probably GitHub :","code":"install.packages(\"probably\") # install.packages(\"pak\") pak::pak(\"tidymodels/probably\")"},{"path":"https://probably.tidymodels.org/dev/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Tools for Post-Processing Class Probability Estimates","text":"Good places look examples using probably vignettes. vignette(\"equivocal-zones\", \"probably\") discusses new class_pred class probably provides working equivocal zones. vignette(\"--use\", \"probably\") discusses probably fits rest tidymodels ecosystem, provides example optimizing class probability thresholds.","code":""},{"path":"https://probably.tidymodels.org/dev/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tools for Post-Processing Class Probability Estimates","text":"project released Contributor Code Conduct. contributing project, agree abide terms. questions discussions tidymodels packages, modeling, machine learning, please post RStudio Community. think encountered bug, please submit issue. Either way, learn create share reprex (minimal, reproducible example), clearly communicate code. Check details contributing guidelines tidymodels packages get help.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/append_class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Add a class_pred column — append_class_pred","title":"Add a class_pred column — append_class_pred","text":"function similar make_class_pred(), useful large number class probability columns want use tidyselect helpers. appends new class_pred vector column original data frame.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/append_class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add a class_pred column — append_class_pred","text":"","code":"append_class_pred(   .data,   ...,   levels,   ordered = FALSE,   min_prob = 1/length(levels),   name = \".class_pred\" )"},{"path":"https://probably.tidymodels.org/dev/reference/append_class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add a class_pred column — append_class_pred","text":".data data frame tibble. ... One unquoted expressions separated commas capture columns .data containing class probabilities. can treat variable names like positions, can use expressions like x:y select ranges variables use selector functions choose columns. make_class_pred, columns class probabilities selected (order levels object). two_class_pred, vector class probabilities selected. levels character vector class levels. length number selections made ..., length 2 make_two_class_pred(). ordered single logical determine levels regarded ordered (order given). results class_pred object flagged ordered. min_prob single numeric value. probabilities less value (row), row marked equivocal. name single character value name appended class_pred column.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/append_class_pred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add a class_pred column — append_class_pred","text":".data extra class_pred column appended onto .","code":""},{"path":"https://probably.tidymodels.org/dev/reference/append_class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add a class_pred column — append_class_pred","text":"","code":"# The following two examples are equivalent and demonstrate # the helper, append_class_pred()  library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  species_probs %>%   mutate(     .class_pred = make_class_pred(       .pred_bobcat, .pred_coyote, .pred_gray_fox,       levels = levels(Species),       min_prob = .5     )   ) #> # A tibble: 110 × 5 #>    Species  .pred_bobcat .pred_coyote .pred_gray_fox .class_pred #>    <fct>           <dbl>        <dbl>          <dbl>  <clss_prd> #>  1 gray_fox       0.0976       0.0530         0.849     gray_fox #>  2 gray_fox       0.155        0.139          0.706     gray_fox #>  3 bobcat         0.501        0.0880         0.411       bobcat #>  4 gray_fox       0.256        0              0.744     gray_fox #>  5 gray_fox       0.463        0.287          0.250         [EQ] #>  6 bobcat         0.811        0              0.189       bobcat #>  7 bobcat         0.911        0.0888         0           bobcat #>  8 bobcat         0.898        0.0517         0.0500      bobcat #>  9 bobcat         0.771        0.229          0           bobcat #> 10 bobcat         0.623        0.325          0.0517      bobcat #> # ℹ 100 more rows  lvls <- levels(species_probs$Species)  append_class_pred(   .data = species_probs,   contains(\".pred_\"),   levels = lvls,   min_prob = .5 ) #> # A tibble: 110 × 5 #>    Species  .pred_bobcat .pred_coyote .pred_gray_fox .class_pred #>    <fct>           <dbl>        <dbl>          <dbl>  <clss_prd> #>  1 gray_fox       0.0976       0.0530         0.849     gray_fox #>  2 gray_fox       0.155        0.139          0.706     gray_fox #>  3 bobcat         0.501        0.0880         0.411       bobcat #>  4 gray_fox       0.256        0              0.744     gray_fox #>  5 gray_fox       0.463        0.287          0.250         [EQ] #>  6 bobcat         0.811        0              0.189       bobcat #>  7 bobcat         0.911        0.0888         0           bobcat #>  8 bobcat         0.898        0.0517         0.0500      bobcat #>  9 bobcat         0.771        0.229          0           bobcat #> 10 bobcat         0.623        0.325          0.0517      bobcat #> # ℹ 100 more rows"},{"path":"https://probably.tidymodels.org/dev/reference/as_class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce to a class_pred object — as_class_pred","title":"Coerce to a class_pred object — as_class_pred","text":"as_class_pred() provides coercion class_pred existing objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/as_class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce to a class_pred object — as_class_pred","text":"","code":"as_class_pred(x, which = integer(), equivocal = \"[EQ]\")"},{"path":"https://probably.tidymodels.org/dev/reference/as_class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce to a class_pred object — as_class_pred","text":"x factor ordered factor. integer vector specifying locations x declare equivocal. equivocal single character specifying equivocal label used printing.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/as_class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coerce to a class_pred object — as_class_pred","text":"","code":"x <- factor(c(\"Yes\", \"No\", \"Yes\", \"Yes\")) as_class_pred(x) #> [1] Yes No  Yes Yes #> Levels: No Yes #> Reportable: 100%"},{"path":"https://probably.tidymodels.org/dev/reference/boosting_predictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Boosted regression trees predictions — boosting_predictions","title":"Boosted regression trees predictions — boosting_predictions","text":"Boosted regression trees predictions","code":""},{"path":"https://probably.tidymodels.org/dev/reference/boosting_predictions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Boosted regression trees predictions — boosting_predictions","text":"boosting_predictions_oob,boosting_predictions_test tibbles","code":""},{"path":"https://probably.tidymodels.org/dev/reference/boosting_predictions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Boosted regression trees predictions — boosting_predictions","text":"data set holdout predictions 10-fold cross-validation separate collection test set predictions boosted tree model. data generated using sim_regression function modeldata package.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/boosting_predictions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Boosted regression trees predictions — boosting_predictions","text":"","code":"data(boosting_predictions_oob) #> Warning: data set ‘boosting_predictions_oob’ not found str(boosting_predictions_oob) #> tibble [2,000 × 3] (S3: tbl_df/tbl/data.frame) #>  $ outcome: num [1:2000] -13.45 43.85 6.14 26.85 -7.43 ... #>  $ .pred  : num [1:2000] 3.13 32.37 10.3 17.79 12.28 ... #>  $ id     : chr [1:2000] \"Fold01\" \"Fold01\" \"Fold01\" \"Fold01\" ... str(boosting_predictions_test) #> tibble [500 × 2] (S3: tbl_df/tbl/data.frame) #>  $ outcome: num [1:500] -4.65 1.12 14.7 36.28 14.08 ... #>  $ .pred  : num [1:500] 4.12 1.83 13.05 19.07 14.93 ..."},{"path":"https://probably.tidymodels.org/dev/reference/cal_apply.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies a calibration to a set of existing predictions — cal_apply","title":"Applies a calibration to a set of existing predictions — cal_apply","text":"Applies calibration set existing predictions","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_apply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies a calibration to a set of existing predictions — cal_apply","text":"","code":"cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)  # S3 method for data.frame cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)  # S3 method for tune_results cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)  # S3 method for cal_object cal_apply(.data, object, pred_class = NULL, parameters = NULL, ...)"},{"path":"https://probably.tidymodels.org/dev/reference/cal_apply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies a calibration to a set of existing predictions — cal_apply","text":".data object can process calibration object. object calibration object (cal_object). pred_class (Optional, classification ) Column identifier hard class predictions (factor vector). column adjusted based changes calibrated probability columns. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Optional arguments; currently unused.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_apply.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Applies a calibration to a set of existing predictions — cal_apply","text":"cal_apply() currently supports data.frames . extracts truth estimate columns names calibration object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_apply.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Applies a calibration to a set of existing predictions — cal_apply","text":"","code":"# ------------------------------------------------------------------------------ # classification example  w_calibration <- cal_estimate_logistic(segment_logistic, Class)  cal_apply(segment_logistic, w_calibration) #> # A tibble: 1,010 × 3 #>    .pred_poor .pred_good Class #>         <dbl>      <dbl> <fct> #>  1      0.974     0.0258 poor  #>  2      0.930     0.0700 poor  #>  3      0.220     0.780  good  #>  4      0.205     0.795  good  #>  5      0.976     0.0244 poor  #>  6      0.590     0.410  good  #>  7      0.777     0.223  good  #>  8      0.135     0.865  good  #>  9      0.977     0.0231 poor  #> 10      0.770     0.230  poor  #> # ℹ 1,000 more rows"},{"path":"https://probably.tidymodels.org/dev/reference/cal_binary_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability Calibration table — .cal_table_breaks","title":"Probability Calibration table — .cal_table_breaks","text":"Calibration table functions. require data.frame contains predictions probability columns. output another tibble segmented data compares accuracy probability actual outcome.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_binary_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability Calibration table — .cal_table_breaks","text":"","code":".cal_table_breaks(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   num_breaks = 10,   conf_level = 0.9,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  .cal_table_logistic(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   conf_level = 0.9,   smooth = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  .cal_table_windowed(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   window_size = 0.1,   step_size = window_size/2,   conf_level = 0.9,   event_level = c(\"auto\", \"first\", \"second\"),   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_binary_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability Calibration table — .cal_table_breaks","text":".data data.frame object containing predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. group column identifier group results. num_breaks number segments group probabilities. defaults 10. conf_level Confidence level use visualization. defaults 0.9. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". Defaults \"auto\", allows function decide one use based type model (binary, multi-class linear) ... Additional arguments passed tune_results object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_binary_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probability Calibration table — .cal_table_breaks","text":".cal_table_breaks() - Splits data bins, based number breaks provided (num_breaks). bins even ranges, starting 0, ending 1. .cal_table_logistic() - Fits logistic spline regression (GAM) data. creates table predictions based 100 probabilities starting 0, ending 1. .cal_table_windowed() - Creates running percentage probability moves across proportion events.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_binary_tables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability Calibration table — .cal_table_breaks","text":"","code":".cal_table_breaks(   segment_logistic,   Class,   .pred_good ) #> # A tibble: 10 × 6 #>    predicted_midpoint event_rate events total  lower  upper #>                 <dbl>      <dbl>  <dbl> <int>  <dbl>  <dbl> #>  1               0.05     0.0350     12   343 0.0208 0.0570 #>  2               0.15     0.0841      9   107 0.0461 0.145  #>  3               0.25     0.324      24    74 0.236  0.426  #>  4               0.35     0.366      26    71 0.272  0.471  #>  5               0.45     0.538      28    52 0.416  0.656  #>  6               0.55     0.473      26    55 0.357  0.591  #>  7               0.65     0.491      27    55 0.374  0.608  #>  8               0.75     0.691      38    55 0.572  0.790  #>  9               0.85     0.722      70    97 0.636  0.794  #> 10               0.95     0.851      86   101 0.779  0.905   .cal_table_logistic(   segment_logistic,   Class,   .pred_good ) #> # A tibble: 101 × 4 #>    estimate   prob  lower  upper #>       <dbl>  <dbl>  <dbl>  <dbl> #>  1     0    0.0219 0.0143 0.0335 #>  2     0.01 0.0246 0.0165 0.0365 #>  3     0.02 0.0276 0.0190 0.0399 #>  4     0.03 0.0310 0.0219 0.0437 #>  5     0.04 0.0347 0.0250 0.0479 #>  6     0.05 0.0389 0.0286 0.0527 #>  7     0.06 0.0435 0.0325 0.0580 #>  8     0.07 0.0487 0.0369 0.0640 #>  9     0.08 0.0544 0.0418 0.0706 #> 10     0.09 0.0608 0.0472 0.0780 #> # ℹ 91 more rows  .cal_table_windowed(   segment_logistic,   Class,   .pred_good ) #> # A tibble: 21 × 6 #>    predicted_midpoint event_rate events total  lower  upper #>                 <dbl>      <dbl>  <dbl> <int>  <dbl>  <dbl> #>  1              0.025     0.0233      6   258 0.0108 0.0468 #>  2              0.05      0.0350     12   343 0.0208 0.0570 #>  3              0.1       0.0559      8   143 0.0293 0.101  #>  4              0.15      0.0841      9   107 0.0461 0.145  #>  5              0.2       0.195      17    87 0.130  0.280  #>  6              0.25      0.324      24    74 0.236  0.426  #>  7              0.3       0.343      24    70 0.251  0.448  #>  8              0.35      0.366      26    71 0.272  0.471  #>  9              0.4       0.433      29    67 0.331  0.540  #> 10              0.45      0.538      28    52 0.416  0.656  #> # ℹ 11 more rows"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":"Uses Beta calibration model calculate new probabilities","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":"","code":"cal_estimate_beta(   .data,   truth = NULL,   shape_params = 2,   location_params = 1,   estimate = dplyr::starts_with(\".pred_\"),   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_beta(   .data,   truth = NULL,   shape_params = 2,   location_params = 1,   estimate = dplyr::starts_with(\".pred_\"),   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_beta(   .data,   truth = NULL,   shape_params = 2,   location_params = 1,   estimate = dplyr::starts_with(\".pred_\"),   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. shape_params Number shape parameters use. Accepted values 1 2. Defaults 2. location_params Number location parameters use. Accepted values 1 0. Defaults 1. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new probabilities.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":"function uses betcal::beta_calibration() function, retains resulting model.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":"Meelis Kull, Telmo M. Silva Filho, Peter Flach \"Beyond sigmoids: obtain well-calibrated probabilities binary classifiers beta calibration,\" Electronic Journal Statistics 11(2), 5052-5080, (2017)","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_beta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses a Beta calibration model to calculate new probabilities — cal_estimate_beta","text":"","code":"# It will automatically identify the probability columns # if passed a model fitted with tidymodels cal_estimate_beta(segment_logistic, Class) #>  #> ── Probability Calibration  #> Method: Beta #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":"Uses Isotonic regression model calibrate model predictions.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":"","code":"cal_estimate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new probabilities.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":"function uses stats::isoreg() create obtain calibration values binary classification numeric regression.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":"Zadrozny, Bianca Elkan, Charles. (2002). Transforming Classifier Scores Accurate Multiclass Probability Estimates. Proceedings ACM SIGKDD International Conference Knowledge Discovery Data Mining.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses an Isotonic regression model to calibrate model predictions. — cal_estimate_isotonic","text":"","code":"# ------------------------------------------------------------------------------ # Binary Classification  # It will automatically identify the probability columns # if passed a model fitted with tidymodels cal_estimate_isotonic(segment_logistic, Class) #>  #> ── Probability Calibration  #> Method: Isotonic #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Unique Predicted Values: 81 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor  # Specify the variable names in a vector of unquoted names cal_estimate_isotonic(segment_logistic, Class, c(.pred_poor, .pred_good)) #>  #> ── Probability Calibration  #> Method: Isotonic #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Unique Predicted Values: 70 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor  # dplyr selector functions are also supported cal_estimate_isotonic(segment_logistic, Class, dplyr::starts_with(\".pred_\")) #>  #> ── Probability Calibration  #> Method: Isotonic #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Unique Predicted Values: 79 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor  # ------------------------------------------------------------------------------ # Regression (numeric outcomes)  cal_estimate_isotonic(boosting_predictions_oob, outcome, .pred) #>  #> ── Probability Calibration  #> Method: Isotonic #> Type: Regression #> Source class: Data Frame #> Data points: 2,000 #> Unique Predicted Values: 46 #> Truth variable: `outcome` #> Estimate variables: #> `.pred` ==> predictions"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic_boot.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","title":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","text":"Uses bootstrapped Isotonic regression model calibrate probabilities","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic_boot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","text":"","code":"cal_estimate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   times = 10,   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   times = 10,   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   times = 10,   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic_boot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. times Number bootstraps. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new probabilities.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic_boot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","text":"function uses stats::isoreg() create obtain calibration values. runs isoreg() multiple times, time different seed. results saved inside returned cal_object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_isotonic_boot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses a bootstrapped Isotonic regression model to calibrate probabilities — cal_estimate_isotonic_boot","text":"","code":"# It will automatically identify the probability columns # if passed a model fitted with tidymodels cal_estimate_isotonic_boot(segment_logistic, Class) #>  #> ── Probability Calibration  #> Method: Bootstrapped Isotonic Regression #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor # Specify the variable names in a vector of unquoted names cal_estimate_isotonic_boot(segment_logistic, Class, c(.pred_poor, .pred_good)) #>  #> ── Probability Calibration  #> Method: Bootstrapped Isotonic Regression #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor # dplyr selector functions are also supported cal_estimate_isotonic_boot(segment_logistic, Class, dplyr::starts_with(\".pred\")) #>  #> ── Probability Calibration  #> Method: Bootstrapped Isotonic Regression #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","title":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","text":"Uses linear regression model calibrate numeric predictions","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","text":"","code":"cal_estimate_linear(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_linear(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_linear(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier observed outcome data (numeric). unquoted column name. estimate Column identifier predicted values smooth Applies linear models. switches generalized additive model using spline terms TRUE, simple linear regression FALSE. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new predictions.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_linear.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","text":"function uses existing modeling functions packages create calibration: stats::glm() used smooth set FALSE mgcv::gam() used smooth set TRUE methods estimate relationship unmodified predicted values remove trend cal_apply() invoked.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_linear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses a linear regression model to calibrate numeric predictions — cal_estimate_linear","text":"","code":"library(dplyr) library(ggplot2)  head(boosting_predictions_test) #> # A tibble: 6 × 2 #>   outcome .pred #>     <dbl> <dbl> #> 1   -4.65  4.12 #> 2    1.12  1.83 #> 3   14.7  13.1  #> 4   36.3  19.1  #> 5   14.1  14.9  #> 6   -4.22  8.10  # ------------------------------------------------------------------------------ # Before calibration  y_rng <- extendrange(boosting_predictions_test$outcome)  boosting_predictions_test %>%   ggplot(aes(outcome, .pred)) +   geom_abline(lty = 2) +   geom_point(alpha = 1 / 2) +   geom_smooth(se = FALSE, col = \"blue\", linewidth = 1.2, alpha = 3 / 4) +   coord_equal(xlim = y_rng, ylim = y_rng) +   ggtitle(\"Before calibration\") #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'   # ------------------------------------------------------------------------------ # Smoothed trend removal  smoothed_cal <-   boosting_predictions_oob %>%   # It will automatically identify the predicted value columns when the   # standard tidymodels naming conventions are used.   cal_estimate_linear(outcome) smoothed_cal #>  #> ── Regression Calibration  #> Method: Linear Spline #> Source class: Data Frame #> Data points: 2,000 #> Truth variable: `outcome` #> Estimate variable: `.pred`  boosting_predictions_test %>%   cal_apply(smoothed_cal) %>%   ggplot(aes(outcome, .pred)) +   geom_abline(lty = 2) +   geom_point(alpha = 1 / 2) +   geom_smooth(se = FALSE, col = \"blue\", linewidth = 1.2, alpha = 3 / 4) +   coord_equal(xlim = y_rng, ylim = y_rng) +   ggtitle(\"After calibration\") #> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_logistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","title":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","text":"Uses logistic regression model calibrate probabilities","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_logistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","text":"","code":"cal_estimate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_logistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. smooth Applies logistic models. switches logistic spline TRUE, simple logistic regression FALSE. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new probabilities.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_logistic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","text":"function uses existing modeling functions packages create calibration: stats::glm() used smooth set FALSE mgcv::gam() used smooth set TRUE","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_logistic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses a logistic regression model to calibrate probabilities — cal_estimate_logistic","text":"","code":"# It will automatically identify the probability columns # if passed a model fitted with tidymodels cal_estimate_logistic(segment_logistic, Class) #>  #> ── Probability Calibration  #> Method: Logistic Spline #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor # Specify the variable names in a vector of unquoted names cal_estimate_logistic(segment_logistic, Class, c(.pred_poor, .pred_good)) #>  #> ── Probability Calibration  #> Method: Logistic Spline #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor # dplyr selector functions are also supported cal_estimate_logistic(segment_logistic, Class, dplyr::starts_with(\".pred_\")) #>  #> ── Probability Calibration  #> Method: Logistic Spline #> Type: Binary #> Source class: Data Frame #> Data points: 1,010 #> Truth variable: `Class` #> Estimate variables: #> `.pred_good` ==> good #> `.pred_poor` ==> poor"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_multinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","title":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","text":"Uses Multinomial calibration model calculate new probabilities","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_multinomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","text":"","code":"cal_estimate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for data.frame cal_estimate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for tune_results cal_estimate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   smooth = TRUE,   parameters = NULL,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_multinomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. smooth Applies logistic models. switches logistic spline TRUE, simple logistic regression FALSE. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new probabilities.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_multinomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","text":"smooth = FALSE, nnet::multinom() function used estimate model, otherwise mgcv::gam() used.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_estimate_multinomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uses a Multinomial calibration model to calculate new probabilities — cal_estimate_multinomial","text":"","code":"library(modeldata) library(parsnip) library(dplyr)  f <-   list(     ~ -0.5 + 0.6 * abs(A),     ~ ifelse(A > 0 & B > 0, 1.0 + 0.2 * A / B, -2),     ~ -0.6 * A + 0.50 * B - A * B   )  set.seed(1) tr_dat  <- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]]) cal_dat <- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]]) te_dat  <- sim_multinomial(500, eqn_1 = f[[1]], eqn_2 = f[[2]], eqn_3 = f[[3]])  set.seed(2) rf_fit <-   rand_forest() %>%   set_mode(\"classification\") %>%   set_engine(\"randomForest\") %>%   fit(class ~ ., data = tr_dat)  cal_pred <-   predict(rf_fit, cal_dat, type = \"prob\") %>%   bind_cols(cal_dat) te_pred <-   predict(rf_fit, te_dat, type = \"prob\") %>%   bind_cols(te_dat)  cal_plot_windowed(cal_pred, truth = class, window_size = 0.1, step_size = 0.03)   smoothed_mn <- cal_estimate_multinomial(cal_pred, truth = class)  new_test_pred <- cal_apply(te_pred, smoothed_mn)  cal_plot_windowed(new_test_pred, truth = class, window_size = 0.1, step_size = 0.03)"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_breaks.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability calibration plots via binning — cal_plot_breaks","title":"Probability calibration plots via binning — cal_plot_breaks","text":"plot created assess whether observed rate event predicted probability event model. sequence even, mutually exclusive bins created zero one. bin, data whose predicted probability falls within range bin used calculate observed event rate (along confidence intervals event rate). predictions well calibrated, fitted curve align diagonal line.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_breaks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability calibration plots via binning — cal_plot_breaks","text":"","code":"cal_plot_breaks(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   num_breaks = 10,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for data.frame cal_plot_breaks(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   num_breaks = 10,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for tune_results cal_plot_breaks(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   num_breaks = 10,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_breaks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability calibration plots via binning — cal_plot_breaks","text":".data data.frame object containing predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. group column identifier group results. num_breaks number segments group probabilities. defaults 10. conf_level Confidence level use visualization. defaults 0.9. include_ribbon Flag indicates ribbon layer included. defaults TRUE. include_rug Flag indicates Rug layer included. defaults TRUE. plot, top side shows frequency event occurring, bottom frequency event occurring. include_points Flag indicates point layer included. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". Defaults \"auto\", allows function decide one use based type model (binary, multi-class linear) ... Additional arguments passed tune_results object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_breaks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability calibration plots via binning — cal_plot_breaks","text":"ggplot object.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_breaks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability calibration plots via binning — cal_plot_breaks","text":"","code":"library(ggplot2) library(dplyr)  cal_plot_breaks(   segment_logistic,   Class,   .pred_good )   cal_plot_logistic(   segment_logistic,   Class,   .pred_good )   cal_plot_windowed(   segment_logistic,   Class,   .pred_good )   # The functions support dplyr groups  model <- glm(Class ~ .pred_good, segment_logistic, family = \"binomial\")  preds <- predict(model, segment_logistic, type = \"response\")  gl <- segment_logistic %>%   mutate(.pred_good = 1 - preds, source = \"glm\")  combined <- bind_rows(mutate(segment_logistic, source = \"original\"), gl)  combined %>%   group_by(source) %>%   cal_plot_logistic(Class, .pred_good)   # The grouping can be faceted in ggplot2 combined %>%   group_by(source) %>%   cal_plot_logistic(Class, .pred_good) +   facet_wrap(~source) +   theme(legend.position = \"\")"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_logistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability calibration plots via logistic regression — cal_plot_logistic","title":"Probability calibration plots via logistic regression — cal_plot_logistic","text":"logistic regression model fit original outcome data used outcome estimated class probabilities one class used predictor. smooth = TRUE, generalized additive model fit using mgcv::gam() default smoothing method. Otherwise, simple logistic regression used. predictions well calibrated, fitted curve align diagonal line. Confidence intervals fitted line also shown.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_logistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability calibration plots via logistic regression — cal_plot_logistic","text":"","code":"cal_plot_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   conf_level = 0.9,   smooth = TRUE,   include_rug = TRUE,   include_ribbon = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for data.frame cal_plot_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   conf_level = 0.9,   smooth = TRUE,   include_rug = TRUE,   include_ribbon = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for tune_results cal_plot_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   conf_level = 0.9,   smooth = TRUE,   include_rug = TRUE,   include_ribbon = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_logistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability calibration plots via logistic regression — cal_plot_logistic","text":".data data.frame object containing predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. group column identifier group results. conf_level Confidence level use visualization. defaults 0.9. smooth logical using generalized additive model smooth terms predictor via mgcv::gam() mgcv::s(). include_rug Flag indicates Rug layer included. defaults TRUE. plot, top side shows frequency event occurring, bottom frequency event occurring. include_ribbon Flag indicates ribbon layer included. defaults TRUE. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". Defaults \"auto\", allows function decide one use based type model (binary, multi-class linear) ... Additional arguments passed tune_results object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_logistic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability calibration plots via logistic regression — cal_plot_logistic","text":"ggplot object.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_logistic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability calibration plots via logistic regression — cal_plot_logistic","text":"","code":"library(ggplot2) library(dplyr)  cal_plot_logistic(   segment_logistic,   Class,   .pred_good )   cal_plot_logistic(   segment_logistic,   Class,   .pred_good,   smooth = FALSE )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression calibration plots — cal_plot_regression","title":"Regression calibration plots — cal_plot_regression","text":"scatter plot observed predicted values computed axes . smooth = TRUE, generalized additive model fit shown. predictions well calibrated, fitted curve align diagonal line.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression calibration plots — cal_plot_regression","text":"","code":"cal_plot_regression(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   smooth = TRUE,   ... )  # S3 method for data.frame cal_plot_regression(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   smooth = TRUE,   ... )  # S3 method for tune_results cal_plot_regression(   .data,   truth = NULL,   estimate = NULL,   group = NULL,   smooth = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression calibration plots — cal_plot_regression","text":".data data.frame object containing prediction truth columns. truth column identifier true results (numeric). unquoted column name. estimate column identifier predictions. unquoted column name group column identifier group results. numeric variable. smooth logical: smoother curve added. ... Additional arguments passed ggplot2::geom_point().","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression calibration plots — cal_plot_regression","text":"ggplot object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression calibration plots — cal_plot_regression","text":"","code":"cal_plot_regression(boosting_predictions_oob, outcome, .pred)   cal_plot_regression(boosting_predictions_oob, outcome, .pred,                     alpha = 1 / 6, cex = 3, smooth = FALSE)   cal_plot_regression(boosting_predictions_oob, outcome, .pred, group = id,                     alpha = 1 / 6, cex = 3, smooth = FALSE)"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_windowed.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability calibration plots via moving windows — cal_plot_windowed","title":"Probability calibration plots via moving windows — cal_plot_windowed","text":"plot created assess whether observed rate event sample predicted probability event model. similar cal_plot_breaks(), except bins overlapping. sequence bins created zero one. bin, data whose predicted probability falls within range bin used calculate observed event rate (along confidence intervals event rate). predictions well calibrated, fitted curve align diagonal line.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_windowed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability calibration plots via moving windows — cal_plot_windowed","text":"","code":"cal_plot_windowed(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   window_size = 0.1,   step_size = window_size/2,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for data.frame cal_plot_windowed(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   window_size = 0.1,   step_size = window_size/2,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )  # S3 method for tune_results cal_plot_windowed(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   group = NULL,   window_size = 0.1,   step_size = window_size/2,   conf_level = 0.9,   include_ribbon = TRUE,   include_rug = TRUE,   include_points = TRUE,   event_level = c(\"auto\", \"first\", \"second\"),   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_windowed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability calibration plots via moving windows — cal_plot_windowed","text":".data data.frame object containing predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. group column identifier group results. window_size size segments. Used windowed probability calculations. defaults 10% segments. step_size gap segments. Used windowed probability calculations. defaults half size window_size conf_level Confidence level use visualization. defaults 0.9. include_ribbon Flag indicates ribbon layer included. defaults TRUE. include_rug Flag indicates Rug layer included. defaults TRUE. plot, top side shows frequency event occurring, bottom frequency event occurring. include_points Flag indicates point layer included. event_level single string. Either \"first\" \"second\" specify level truth consider \"event\". Defaults \"auto\", allows function decide one use based type model (binary, multi-class linear) ... Additional arguments passed tune_results object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_windowed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability calibration plots via moving windows — cal_plot_windowed","text":"ggplot object.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_plot_windowed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability calibration plots via moving windows — cal_plot_windowed","text":"","code":"library(ggplot2) library(dplyr)  cal_plot_windowed(   segment_logistic,   Class,   .pred_good )   # More breaks cal_plot_windowed(   segment_logistic,   Class,   .pred_good,   window_size = 0.05 )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using Beta calibration — cal_validate_beta","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":"function uses resampling measure effect calibrating predicted values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":"","code":"cal_validate_beta(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   summarize = TRUE,   save_details = FALSE,   ... )  # S3 method for resample_results cal_validate_beta(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   summarize = TRUE,   save_details = FALSE,   ... )  # S3 method for rset cal_validate_beta(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   summarize = TRUE,   save_details = FALSE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":".data rset object results tune::fit_resamples() .predictions column. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. save_details Indicates whether include calibration validation columns summarize argument set FALSE. ... Options pass cal_estimate_beta(), shape_params location_params arguments.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":"functions designed calculate performance without calibration. use resampling measure --sample effectiveness. two ways pass data : data frame predictions, rset object can created via rsample functions. See example . already made resampling object original data used tune::fit_resamples(), can pass object calibration function use resampling scheme. different resampling scheme used, run tune::collect_predictions() object use process previous bullet point. Please note functions apply tune_result objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":"default, average Brier scores returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_beta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using Beta calibration — cal_validate_beta","text":"","code":"library(dplyr)  segment_logistic %>%   rsample::vfold_cv() %>%   cal_validate_beta(Class) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class binary     minimize  uncalibrated     0.140 #> 2 brier_class binary     minimize  calibrated       0.138"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":"function uses resampling measure effect calibrating predicted values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":"","code":"cal_validate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for resample_results cal_validate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for rset cal_validate_isotonic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":".data rset object results tune::fit_resamples() .predictions column. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() save_details Indicates whether include calibration validation columns summarize argument set FALSE. summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. ... Options pass cal_estimate_logistic(), smooth argument.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":"functions designed calculate performance without calibration. use resampling measure --sample effectiveness. two ways pass data : data frame predictions, rset object can created via rsample functions. See example . already made resampling object original data used tune::fit_resamples(), can pass object calibration function use resampling scheme. different resampling scheme used, run tune::collect_predictions() object use process previous bullet point. Please note functions apply tune_result objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":"default, average Brier scores (classification calibration) root mean squared error (regression) returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using isotonic regression calibration — cal_validate_isotonic","text":"","code":"library(dplyr)  segment_logistic %>%   rsample::vfold_cv() %>%   cal_validate_isotonic(Class) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class binary     minimize  uncalibrated     0.140 #> 2 brier_class binary     minimize  calibrated       0.148"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":"function uses resampling measure effect calibrating predicted values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":"","code":"cal_validate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for resample_results cal_validate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for rset cal_validate_isotonic_boot(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":".data rset object results tune::fit_resamples() .predictions column. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() save_details Indicates whether include calibration validation columns summarize argument set FALSE. summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. ... Options pass cal_estimate_isotonic_boot(), times argument.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":"functions designed calculate performance without calibration. use resampling measure --sample effectiveness. two ways pass data : data frame predictions, rset object can created via rsample functions. See example . already made resampling object original data used tune::fit_resamples(), can pass object calibration function use resampling scheme. different resampling scheme used, run tune::collect_predictions() object use process previous bullet point. Please note functions apply tune_result objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":"default, average Brier scores (classification calibration) root mean squared error (regression) returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_isotonic_boot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using bagged isotonic regression calibration — cal_validate_isotonic_boot","text":"","code":"library(dplyr)  segment_logistic %>%   rsample::vfold_cv() %>%   cal_validate_isotonic_boot(Class) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class binary     minimize  uncalibrated     0.140 #> 2 brier_class binary     minimize  calibrated       0.142"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using linear regression calibration — cal_validate_linear","title":"Measure performance with and without using linear regression calibration — cal_validate_linear","text":"Measure performance without using linear regression calibration","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using linear regression calibration — cal_validate_linear","text":"","code":"cal_validate_linear(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for resample_results cal_validate_linear(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for rset cal_validate_linear(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using linear regression calibration — cal_validate_linear","text":".data data.frame object, tune_results object, contains predictions probability columns. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() save_details Indicates whether include calibration validation columns summarize argument set FALSE. summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. ... Options pass cal_estimate_linear(), smooth argument.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_linear.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using linear regression calibration — cal_validate_linear","text":"default, average root mean square error (RMSE) returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_linear.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using linear regression calibration — cal_validate_linear","text":"","code":"library(dplyr) library(yardstick) library(rsample)  head(boosting_predictions_test) #> # A tibble: 6 × 2 #>   outcome .pred #>     <dbl> <dbl> #> 1   -4.65  4.12 #> 2    1.12  1.83 #> 3   14.7  13.1  #> 4   36.3  19.1  #> 5   14.1  14.9  #> 6   -4.22  8.10  reg_stats <- metric_set(rmse, ccc)  set.seed(828) boosting_predictions_oob %>%   # Resample with 10-fold cross-validation   vfold_cv() %>%   cal_validate_linear(truth = outcome, smooth = FALSE, metrics = reg_stats) #> # A tibble: 4 × 5 #>   .metric .estimator direction stage        .estimate #>   <chr>   <chr>      <chr>     <chr>            <dbl> #> 1 rmse    standard   minimize  uncalibrated    15.9   #> 2 rmse    standard   minimize  calibrated      13.9   #> 3 ccc     standard   maximize  uncalibrated     0.461 #> 4 ccc     standard   maximize  calibrated       0.696"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using logistic calibration — cal_validate_logistic","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":"function uses resampling measure effect calibrating predicted values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":"","code":"cal_validate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for resample_results cal_validate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for rset cal_validate_logistic(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":".data rset object results tune::fit_resamples() .predictions column. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() save_details Indicates whether include calibration validation columns summarize argument set FALSE. summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. ... Options pass cal_estimate_logistic(), smooth argument.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":"functions designed calculate performance without calibration. use resampling measure --sample effectiveness. two ways pass data : data frame predictions, rset object can created via rsample functions. See example . already made resampling object original data used tune::fit_resamples(), can pass object calibration function use resampling scheme. different resampling scheme used, run tune::collect_predictions() object use process previous bullet point. Please note functions apply tune_result objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":"default, average Brier scores returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_logistic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using logistic calibration — cal_validate_logistic","text":"","code":"library(dplyr)  # --------------------------------------------------------------------------- # classification example  segment_logistic %>%   rsample::vfold_cv() %>%   cal_validate_logistic(Class) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class binary     minimize  uncalibrated     0.140 #> 2 brier_class binary     minimize  calibrated       0.138"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":"function uses resampling measure effect calibrating predicted values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":"","code":"cal_validate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for resample_results cal_validate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )  # S3 method for rset cal_validate_multinomial(   .data,   truth = NULL,   estimate = dplyr::starts_with(\".pred_\"),   metrics = NULL,   save_details = FALSE,   summarize = TRUE,   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":".data rset object results tune::fit_resamples() .predictions column. truth column identifier true class results (factor). unquoted column name. estimate vector column identifiers, one dplyr selector functions choose variables contains class probabilities. defaults prefix used tidymodels (.pred_). order identifiers considered order levels truth variable. metrics set metrics passed created via yardstick::metric_set() save_details Indicates whether include calibration validation columns summarize argument set FALSE. summarize Indicates pass tibble metrics averaged, return sampled object new columns containing calibration y validation list columns. ... Options pass cal_estimate_logistic(), smooth argument.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":"functions designed calculate performance without calibration. use resampling measure --sample effectiveness. two ways pass data : data frame predictions, rset object can created via rsample functions. See example . already made resampling object original data used tune::fit_resamples(), can pass object calibration function use resampling scheme. different resampling scheme used, run tune::collect_predictions() object use process previous bullet point. Please note functions apply tune_result objects.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":"performance-metrics","dir":"Reference","previous_headings":"","what":"Performance Metrics","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":"default, average Brier scores returned. appropriate yardstick::metric_set() can used. validation function compares average metrics , calibration.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_multinomial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure performance with and without using multinomial calibration — cal_validate_multinomial","text":"","code":"library(dplyr)  species_probs %>%   rsample::vfold_cv() %>%   cal_validate_multinomial(Species) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class multiclass minimize  uncalibrated     0.165 #> 2 brier_class multiclass minimize  calibrated       0.178"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizes the resampling metrics associated with calibration — cal_validate_summarize","title":"Summarizes the resampling metrics associated with calibration — cal_validate_summarize","text":"Summarizes resampling metrics associated calibration","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizes the resampling metrics associated with calibration — cal_validate_summarize","text":"","code":"cal_validate_summarize(x)  # S3 method for cal_rset cal_validate_summarize(x)"},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizes the resampling metrics associated with calibration — cal_validate_summarize","text":"x results one cal_validate_*() functions.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/cal_validate_summarize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizes the resampling metrics associated with calibration — cal_validate_summarize","text":"","code":"library(dplyr)  sl_val <- segment_logistic %>%   rsample::vfold_cv() %>%   cal_validate_beta(Class, summarize = FALSE, save_details = TRUE)  sl_val #> #  10-fold cross-validation  #> # A tibble: 10 × 6 #>    splits            id     calibration  validation         stats_after #>    <list>            <chr>  <list>       <list>             <list>      #>  1 <split [909/101]> Fold01 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  2 <split [909/101]> Fold02 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  3 <split [909/101]> Fold03 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  4 <split [909/101]> Fold04 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  5 <split [909/101]> Fold05 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  6 <split [909/101]> Fold06 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  7 <split [909/101]> Fold07 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  8 <split [909/101]> Fold08 <Beta [909]> <tibble [101 × 4]> <tibble>    #>  9 <split [909/101]> Fold09 <Beta [909]> <tibble [101 × 4]> <tibble>    #> 10 <split [909/101]> Fold10 <Beta [909]> <tibble [101 × 4]> <tibble>    #> # ℹ 1 more variable: stats_before <list>  cal_validate_summarize(sl_val) #> # A tibble: 2 × 5 #>   .metric     .estimator direction stage        .estimate #>   <chr>       <chr>      <chr>     <chr>            <dbl> #> 1 brier_class binary     minimize  uncalibrated     0.140 #> 2 brier_class binary     minimize  calibrated       0.138"},{"path":"https://probably.tidymodels.org/dev/reference/class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a class prediction object — class_pred","title":"Create a class prediction object — class_pred","text":"class_pred() creates class_pred object factor ordered factor. can optionally specify values factor set equivocal.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a class prediction object — class_pred","text":"","code":"class_pred(x = factor(), which = integer(), equivocal = \"[EQ]\")"},{"path":"https://probably.tidymodels.org/dev/reference/class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a class prediction object — class_pred","text":"x factor ordered factor. integer vector specifying locations x declare equivocal. equivocal single character specifying equivocal label used printing.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/class_pred.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a class prediction object — class_pred","text":"Equivocal values feel unsure , like exclude performance calculations metrics.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a class prediction object — class_pred","text":"","code":"x <- factor(c(\"Yes\", \"No\", \"Yes\", \"Yes\"))  # Create a class_pred object from a factor class_pred(x) #> [1] Yes No  Yes Yes #> Levels: No Yes #> Reportable: 100%  # Say you aren't sure about that 2nd \"Yes\" value. You could mark it as # equivocal. class_pred(x, which = 3) #> [1] Yes  No   [EQ] Yes  #> Levels: No Yes #> Reportable: 75%  # Maybe you want a different equivocal label class_pred(x, which = 3, equivocal = \"eq_value\") #> [1] Yes      No       eq_value Yes      #> Levels: No Yes #> Reportable: 75%"},{"path":"https://probably.tidymodels.org/dev/reference/control_conformal_infer.html","id":null,"dir":"Reference","previous_headings":"","what":"Controlling the numeric details for conformal inference — control_conformal_infer","title":"Controlling the numeric details for conformal inference — control_conformal_infer","text":"Controlling numeric details conformal inference","code":""},{"path":"https://probably.tidymodels.org/dev/reference/control_conformal_infer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Controlling the numeric details for conformal inference — control_conformal_infer","text":"","code":"control_conformal_infer(   method = \"iterative\",   trial_points = 100,   var_multiplier = 10,   max_iter = 100,   tolerance = .Machine$double.eps^0.25,   progress = FALSE,   required_pkgs = character(0),   seed = sample.int(10^5, 1) )"},{"path":"https://probably.tidymodels.org/dev/reference/control_conformal_infer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Controlling the numeric details for conformal inference — control_conformal_infer","text":"method method computing intervals. options 'search' (using) stats::uniroot(), 'grid'. trial_points method = \"grid\", many points evaluated? var_multiplier multiplier variance model determines possible range bounds. max_iter method = \"iterative\", maximum number iterations. tolerance Tolerance value passed .equal() determine convergence search computations. progress progress bar used track execution? required_pkgs optional character string packages required. seed single integer used control randomness models (re)fit.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/control_conformal_infer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Controlling the numeric details for conformal inference — control_conformal_infer","text":"list object options given user.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction intervals via conformal inference — int_conformal_infer","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"Nonparametric prediction intervals can computed fitted workflow objects using conformal inference method described Lei al (2018).","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"","code":"int_conformal_infer(object, ...)  # S3 method for default int_conformal_infer(object, ...)  # S3 method for workflow int_conformal_infer(   object,   train_data,   ...,   control = control_conformal_infer() )"},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"object fitted workflows::workflow() object. ... currently used. train_data data frame original predictor data used create fitted workflow (predictors outcomes). workflow contain values, pass . workflow used recipe, data inputs recipe (product recipe). control control object control_conformal_infer() numeric minutiae.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"object class \"int_conformal_infer\" containing information create intervals (includes training set data). predict() method used produce intervals.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"function implements usually called \"full conformal inference\" (see Algorithm 1 Lei et al (2018)) since uses entire training set compute intervals. function prepares objects computations. predict() method computes intervals new data. given new_data observation, predictors appended original training set. , different \"trial\" values outcome substituted observation's outcome model re-fit. model, residual associated trial value compared quantile distribution residuals. Usually absolute values residuals used. residual trial value exceeds distributional quantile, value one bounds. literature proposed using grid search trial values find two points correspond prediction intervals. use approach, set method = \"grid\" control_conformal_infer(). However, default method \"search uses two different one-dimensional iterative searches either side predicted value find values correspond prediction intervals. medium large data sets, iterative search method likely generate slightly smaller intervals. small training sets, grid search likely somewhat smaller intervals (stable). Otherwise, iterative search method precise several folds faster. determine range possible values intervals, used methods, initial set training set residuals modeled using Gamma generalized linear model log link (see reference Aitkin ). new sample, absolute size residual estimated multiple value computed initial guess search boundaries.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"speed","dir":"Reference","previous_headings":"","what":"Speed","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"time takes compute intervals depends training set size, search parameters (.e., convergence criterion, number iterations), grid size, number worker processes used. last item, computations can parallelized using future furrr packages. use parallelism, future::plan() function can invoked create parallel backend. example, let’s make initial workflow:   ’ll use \"multisession\" parallel processing plan compute intervals five new samples parallel:     Using simulations, slightly sub-linear speed-ups using parallel processing compute row-wise intervals. comparison parametric intervals:","code":"library(tidymodels) library(probably) library(future)  tidymodels_prefer()  ## Make a fitted workflow from some simulated data: set.seed(121) train_dat <- sim_regression(200) new_dat   <- sim_regression(  5) %>% select(-outcome)  lm_fit <-    workflow() %>%    add_model(linear_reg()) %>%    add_formula(outcome ~ .) %>%    fit(data = train_dat)  # Create the object to be used to make prediction intervals lm_conform <- int_conformal_infer(lm_fit, train_dat) plan(\"multisession\")  # This is run in parallel: predict(lm_conform, new_dat) ## # A tibble: 5 x 2 ##   .pred_lower .pred_upper ##         <dbl>       <dbl> ## 1       -17.9        59.6 ## 2       -33.7        51.1 ## 3       -30.6        48.2 ## 4       -17.3        59.6 ## 5       -23.3        55.2 predict(lm_fit, new_dat, type = \"pred_int\") ## # A tibble: 5 x 2 ##   .pred_lower .pred_upper ##         <dbl>       <dbl> ## 1       -19.2        59.1 ## 2       -31.8        49.7 ## 3       -31.0        47.6 ## 4       -17.8        60.1 ## 5       -23.6        54.3"},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prediction intervals via conformal inference — int_conformal_infer","text":"Jing Lei, Max G'Sell, Alessandro Rinaldo, Ryan J. Tibshirani Larry Wasserman (2018) Distribution-Free Predictive Inference Regression, Journal American Statistical Association, 113:523, 1094-1111 Murray Aitkin, Modelling Variance Heterogeneity Normal Regression Using GLIM, Journal Royal Statistical Society Series C: Applied Statistics, Volume 36, Issue 3, November 1987, Pages 332–339.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"Nonparametric prediction intervals can computed fitted regression workflow objects using CV+ conformal inference method described Barber al (2018).","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"","code":"int_conformal_infer_cv(object, ...)  # S3 method for default int_conformal_infer_cv(object, ...)  # S3 method for resample_results int_conformal_infer_cv(object, ...)  # S3 method for tune_results int_conformal_infer_cv(object, parameters, ...)"},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"object object tidymodels resampling tuning function tune::fit_resamples(), tune::tune_grid(), similar. object produced way .extracts column contains fitted workflow resample (see Details ). ... currently used. parameters tibble tuning parameter values can used filter predicted values processing. tibble select single set hyper-parameter values tuning results. required tuning object passed object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"object class \"int_conformal_infer_cv\" containing information create intervals. predict() method used produce intervals.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"function implements CV+ method found Section 3 Barber al (2018). uses resampled model fits associated holdout residuals make prediction intervals regression models. function prepares objects computations. predict() method computes intervals new data. method developed V-fold cross-validation (repeats). Interval coverage unknown resampling methods. function stop computations types resamples, way knowing whether results appropriate.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"Rina Foygel Barber, Emmanuel J. Candès, Aaditya Ramdas, Ryan J. Tibshirani \"Predictive inference jackknife+,\" Annals Statistics, 49(1), 486-507, 2021","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/int_conformal_infer_cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prediction intervals via conformal inference CV+ — int_conformal_infer_cv","text":"","code":"library(workflows) library(dplyr) library(parsnip) library(rsample) library(tune) library(modeldata)  set.seed(2) sim_train <- sim_regression(200) sim_new   <- sim_regression(  5) %>% select(-outcome)  sim_rs <- vfold_cv(sim_train)  # We'll use a neural network model mlp_spec <-   mlp(hidden_units = 5, penalty = 0.01) %>%   set_mode(\"regression\")  # Use a control function that saves the predictions as well as the models. # Consider using the butcher package in the extracts function to have smaller # object sizes  ctrl <- control_resamples(save_pred = TRUE, extract = I)  set.seed(3) nnet_res <-   mlp_spec %>%   fit_resamples(outcome ~ ., resamples = sim_rs, control = ctrl)  nnet_int_obj <- int_conformal_infer_cv(nnet_res) nnet_int_obj #> Conformal inference via CV+ #> preprocessor: formula  #> model: mlp (engine = nnet)  #> number of models: 10  #> training set size: 200  #>  #> Use `predict(object, new_data, level)` to compute prediction intervals  predict(nnet_int_obj, sim_new) #> # A tibble: 5 × 2 #>   .pred_lower .pred_upper #>         <dbl>       <dbl> #> 1        3.67        81.9 #> 2      -29.6         48.6 #> 3      -14.6         63.6 #> 4      -39.2         39.0 #> 5      -11.1         67.1"},{"path":"https://probably.tidymodels.org/dev/reference/is_class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if an object inherits from class_pred — is_class_pred","title":"Test if an object inherits from class_pred — is_class_pred","text":"is_class_pred() checks object class_pred object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/is_class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if an object inherits from class_pred — is_class_pred","text":"","code":"is_class_pred(x)"},{"path":"https://probably.tidymodels.org/dev/reference/is_class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if an object inherits from class_pred — is_class_pred","text":"x object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/is_class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test if an object inherits from class_pred — is_class_pred","text":"","code":"x <- class_pred(factor(1:5))  is_class_pred(x) #> [1] TRUE"},{"path":"https://probably.tidymodels.org/dev/reference/levels.class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract class_pred levels — levels.class_pred","title":"Extract class_pred levels — levels.class_pred","text":"levels class_pred object include equivocal value.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/levels.class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract class_pred levels — levels.class_pred","text":"","code":"# S3 method for class_pred levels(x)"},{"path":"https://probably.tidymodels.org/dev/reference/levels.class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract class_pred levels — levels.class_pred","text":"x class_pred object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/levels.class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract class_pred levels — levels.class_pred","text":"","code":"x <- class_pred(factor(1:5), which = 1)  # notice that even though `1` is not in the `class_pred` vector, the # level remains from the original factor levels(x) #> [1] \"1\" \"2\" \"3\" \"4\" \"5\""},{"path":"https://probably.tidymodels.org/dev/reference/locate-equivocal.html","id":null,"dir":"Reference","previous_headings":"","what":"Locate equivocal values — locate-equivocal","title":"Locate equivocal values — locate-equivocal","text":"functions provide multiple methods checking equivocal values, finding locations.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/locate-equivocal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Locate equivocal values — locate-equivocal","text":"","code":"is_equivocal(x)  which_equivocal(x)  any_equivocal(x)"},{"path":"https://probably.tidymodels.org/dev/reference/locate-equivocal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Locate equivocal values — locate-equivocal","text":"x class_pred object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/locate-equivocal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Locate equivocal values — locate-equivocal","text":"is_equivocal() returns logical vector length x TRUE means value equivocal. which_equivocal() returns integer vector specifying locations equivocal values. any_equivocal() returns TRUE equivocal values.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/locate-equivocal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Locate equivocal values — locate-equivocal","text":"","code":"x <- class_pred(factor(1:10), which = c(2, 5))  is_equivocal(x) #>  [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  which_equivocal(x) #> [1] 2 5  any_equivocal(x) #> [1] TRUE"},{"path":"https://probably.tidymodels.org/dev/reference/make_class_pred.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a class_pred vector from class probabilities — make_class_pred","title":"Create a class_pred vector from class probabilities — make_class_pred","text":"functions can used convert class probability estimates class_pred objects optional equivocal zone.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/make_class_pred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a class_pred vector from class probabilities — make_class_pred","text":"","code":"make_class_pred(..., levels, ordered = FALSE, min_prob = 1/length(levels))  make_two_class_pred(   estimate,   levels,   threshold = 0.5,   ordered = FALSE,   buffer = NULL )"},{"path":"https://probably.tidymodels.org/dev/reference/make_class_pred.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a class_pred vector from class probabilities — make_class_pred","text":"... Numeric vectors corresponding class probabilities. one level levels, assumed vectors order levels. levels character vector class levels. length number selections made ..., length 2 make_two_class_pred(). ordered single logical determine levels regarded ordered (order given). results class_pred object flagged ordered. min_prob single numeric value. probabilities less value (row), row marked equivocal. estimate single numeric vector corresponding class probabilities first level levels. threshold single numeric value threshold call row labeled first value levels. buffer numeric vector length 1 2 buffer around threshold defines equivocal zone (.e., threshold - buffer[1] threshold + buffer[2]). length 1 vector recycled length 2. default, NULL, interpreted equivocal zone.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/make_class_pred.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a class_pred vector from class probabilities — make_class_pred","text":"vector class class_pred.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/make_class_pred.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a class_pred vector from class probabilities — make_class_pred","text":"","code":"library(dplyr)  good <- segment_logistic$.pred_good lvls <- levels(segment_logistic$Class)  # Equivocal zone of .5 +/- .15 make_two_class_pred(good, lvls, buffer = 0.15) #>    [1] poor poor good good poor [EQ] poor good poor poor good poor poor #>   [14] good poor good poor poor [EQ] poor poor good poor good poor poor #>   [27] good [EQ] good good poor poor [EQ] good good poor poor poor poor #>   [40] poor [EQ] [EQ] poor poor poor poor poor good good poor [EQ] poor #>   [53] poor [EQ] poor [EQ] poor poor [EQ] poor poor good good poor poor #>   [66] [EQ] good poor [EQ] poor good poor [EQ] poor good poor good [EQ] #>   [79] poor [EQ] good poor poor poor good good poor good poor poor poor #>   [92] poor [EQ] good poor [EQ] good [EQ] [EQ] poor good [EQ] poor poor #>  [105] good good good [EQ] good poor poor poor good poor [EQ] poor [EQ] #>  [118] poor good good poor good poor [EQ] poor poor good good poor poor #>  [131] [EQ] [EQ] poor good good poor [EQ] poor [EQ] [EQ] poor [EQ] [EQ] #>  [144] poor [EQ] poor good [EQ] poor poor poor good good poor poor [EQ] #>  [157] good poor poor [EQ] good poor good good poor poor poor [EQ] [EQ] #>  [170] poor good good poor poor [EQ] good poor poor poor poor [EQ] good #>  [183] poor poor poor [EQ] poor good good poor good [EQ] poor good poor #>  [196] poor good poor poor poor good good [EQ] poor poor poor poor poor #>  [209] [EQ] poor good good poor [EQ] poor poor poor good poor good [EQ] #>  [222] poor good good good poor [EQ] poor poor good poor poor poor good #>  [235] good good poor poor poor [EQ] [EQ] poor poor poor [EQ] poor [EQ] #>  [248] good poor poor poor good poor poor good [EQ] good good poor [EQ] #>  [261] poor good good [EQ] [EQ] good poor poor poor poor [EQ] poor poor #>  [274] poor poor [EQ] good poor [EQ] [EQ] poor poor poor good [EQ] poor #>  [287] [EQ] good poor [EQ] poor good [EQ] good poor poor poor good poor #>  [300] good [EQ] poor poor poor good poor poor poor [EQ] good poor poor #>  [313] good poor good poor [EQ] poor poor [EQ] [EQ] poor poor poor poor #>  [326] good [EQ] poor poor [EQ] poor poor poor poor poor [EQ] good [EQ] #>  [339] poor good poor good [EQ] good poor [EQ] poor poor poor poor poor #>  [352] good good [EQ] [EQ] poor good poor good poor poor poor poor good #>  [365] poor poor poor poor poor poor poor [EQ] poor poor poor poor poor #>  [378] poor poor good poor poor poor poor poor poor [EQ] good poor poor #>  [391] poor [EQ] [EQ] good poor poor poor poor poor poor good [EQ] [EQ] #>  [404] poor poor poor poor poor poor poor good good poor poor poor poor #>  [417] poor [EQ] poor poor poor good [EQ] good good poor poor poor good #>  [430] good good good poor good poor poor poor poor poor poor good [EQ] #>  [443] [EQ] poor good good [EQ] [EQ] poor poor good poor poor good poor #>  [456] good poor poor poor good poor poor poor poor good poor poor good #>  [469] poor good good good poor good poor good good good poor poor good #>  [482] poor poor poor poor poor poor good [EQ] poor [EQ] poor poor poor #>  [495] good poor [EQ] poor [EQ] poor poor poor poor poor poor good good #>  [508] poor [EQ] [EQ] [EQ] poor poor poor poor good [EQ] good poor poor #>  [521] good good poor [EQ] poor poor [EQ] poor good poor poor good poor #>  [534] poor poor poor poor good [EQ] poor good good poor poor good poor #>  [547] good good poor poor good poor good poor [EQ] poor poor poor poor #>  [560] [EQ] good poor good good poor poor poor good good poor poor good #>  [573] [EQ] [EQ] poor [EQ] poor poor poor [EQ] poor good poor good good #>  [586] poor poor poor poor good [EQ] good poor good [EQ] [EQ] poor poor #>  [599] [EQ] [EQ] poor good good good [EQ] good poor poor poor [EQ] poor #>  [612] good poor good [EQ] poor poor poor good good poor good poor poor #>  [625] poor poor poor good poor [EQ] good [EQ] good poor good poor good #>  [638] poor poor [EQ] [EQ] poor poor poor poor poor good good poor poor #>  [651] poor poor poor poor good good poor good poor good poor good poor #>  [664] poor poor [EQ] poor poor good poor poor good good good poor poor #>  [677] poor [EQ] poor good good [EQ] good poor good poor poor poor [EQ] #>  [690] poor poor [EQ] [EQ] good [EQ] poor good poor poor good good poor #>  [703] [EQ] poor good poor poor [EQ] [EQ] [EQ] poor good poor good good #>  [716] good good poor poor poor good poor good poor poor [EQ] poor poor #>  [729] poor poor poor poor [EQ] good good good poor [EQ] poor poor poor #>  [742] good poor good good [EQ] poor good poor [EQ] poor poor poor [EQ] #>  [755] good good poor poor poor good poor good poor good [EQ] poor good #>  [768] [EQ] poor [EQ] good poor good [EQ] poor good poor poor good poor #>  [781] poor good good good poor poor poor poor poor good poor [EQ] poor #>  [794] poor poor good [EQ] poor good [EQ] [EQ] good poor good poor poor #>  [807] poor poor poor poor poor [EQ] poor good poor poor poor poor good #>  [820] poor good good poor poor poor poor poor good [EQ] poor good poor #>  [833] poor poor poor poor poor poor [EQ] poor poor poor poor poor good #>  [846] good good poor poor poor poor poor poor poor poor good [EQ] [EQ] #>  [859] [EQ] poor good [EQ] poor poor poor [EQ] good poor good good poor #>  [872] good poor poor good [EQ] [EQ] [EQ] poor poor poor poor [EQ] good #>  [885] poor good poor good poor poor poor poor good poor poor poor poor #>  [898] poor poor poor poor [EQ] poor poor [EQ] good [EQ] good poor poor #>  [911] poor good [EQ] poor good poor poor poor poor good poor poor good #>  [924] good poor poor good poor [EQ] poor good poor good good good poor #>  [937] poor good good poor poor [EQ] [EQ] poor good poor poor good [EQ] #>  [950] [EQ] poor [EQ] poor good [EQ] [EQ] poor good poor poor poor good #>  [963] poor poor poor poor good poor poor [EQ] poor poor poor good good #>  [976] poor [EQ] poor poor poor good poor poor good poor [EQ] good good #>  [989] good good poor [EQ] poor good poor poor poor poor good good good #> [1002] good good poor good [EQ] good poor poor good #> Levels: good poor #> Reportable: 83.2%  # Equivocal zone of c(.5 - .05, .5 + .15) make_two_class_pred(good, lvls, buffer = c(0.05, 0.15)) #>    [1] poor poor good good poor poor poor good poor poor good poor poor #>   [14] good poor good poor poor [EQ] poor poor good poor good poor poor #>   [27] good poor good good poor poor [EQ] good good poor poor poor poor #>   [40] poor [EQ] poor poor poor poor poor poor good good poor [EQ] poor #>   [53] poor poor poor poor poor poor [EQ] poor poor good good poor poor #>   [66] poor good poor [EQ] poor good poor [EQ] poor good poor good poor #>   [79] poor [EQ] good poor poor poor good good poor good poor poor poor #>   [92] poor [EQ] good poor [EQ] good [EQ] poor poor good [EQ] poor poor #>  [105] good good good poor good poor poor poor good poor poor poor [EQ] #>  [118] poor good good poor good poor [EQ] poor poor good good poor poor #>  [131] [EQ] poor poor good good poor [EQ] poor [EQ] [EQ] poor poor [EQ] #>  [144] poor [EQ] poor good poor poor poor poor good good poor poor poor #>  [157] good poor poor [EQ] good poor good good poor poor poor [EQ] poor #>  [170] poor good good poor poor [EQ] good poor poor poor poor [EQ] good #>  [183] poor poor poor [EQ] poor good good poor good poor poor good poor #>  [196] poor good poor poor poor good good poor poor poor poor poor poor #>  [209] poor poor good good poor [EQ] poor poor poor good poor good [EQ] #>  [222] poor good good good poor [EQ] poor poor good poor poor poor good #>  [235] good good poor poor poor [EQ] poor poor poor poor [EQ] poor poor #>  [248] good poor poor poor good poor poor good [EQ] good good poor [EQ] #>  [261] poor good good [EQ] poor good poor poor poor poor [EQ] poor poor #>  [274] poor poor [EQ] good poor poor [EQ] poor poor poor good [EQ] poor #>  [287] [EQ] good poor poor poor good [EQ] good poor poor poor good poor #>  [300] good [EQ] poor poor poor good poor poor poor [EQ] good poor poor #>  [313] good poor good poor poor poor poor [EQ] [EQ] poor poor poor poor #>  [326] good poor poor poor [EQ] poor poor poor poor poor [EQ] good [EQ] #>  [339] poor good poor good poor good poor [EQ] poor poor poor poor poor #>  [352] good good poor [EQ] poor good poor good poor poor poor poor good #>  [365] poor poor poor poor poor poor poor poor poor poor poor poor poor #>  [378] poor poor good poor poor poor poor poor poor [EQ] good poor poor #>  [391] poor [EQ] [EQ] good poor poor poor poor poor poor good [EQ] [EQ] #>  [404] poor poor poor poor poor poor poor good good poor poor poor poor #>  [417] poor [EQ] poor poor poor good [EQ] good good poor poor poor good #>  [430] good good good poor good poor poor poor poor poor poor good [EQ] #>  [443] poor poor good good [EQ] [EQ] poor poor good poor poor good poor #>  [456] good poor poor poor good poor poor poor poor good poor poor good #>  [469] poor good good good poor good poor good good good poor poor good #>  [482] poor poor poor poor poor poor good poor poor [EQ] poor poor poor #>  [495] good poor poor poor [EQ] poor poor poor poor poor poor good good #>  [508] poor [EQ] [EQ] [EQ] poor poor poor poor good poor good poor poor #>  [521] good good poor poor poor poor [EQ] poor good poor poor good poor #>  [534] poor poor poor poor good poor poor good good poor poor good poor #>  [547] good good poor poor good poor good poor poor poor poor poor poor #>  [560] poor good poor good good poor poor poor good good poor poor good #>  [573] [EQ] poor poor poor poor poor poor poor poor good poor good good #>  [586] poor poor poor poor good [EQ] good poor good poor [EQ] poor poor #>  [599] poor [EQ] poor good good good [EQ] good poor poor poor poor poor #>  [612] good poor good poor poor poor poor good good poor good poor poor #>  [625] poor poor poor good poor poor good poor good poor good poor good #>  [638] poor poor [EQ] [EQ] poor poor poor poor poor good good poor poor #>  [651] poor poor poor poor good good poor good poor good poor good poor #>  [664] poor poor [EQ] poor poor good poor poor good good good poor poor #>  [677] poor [EQ] poor good good [EQ] good poor good poor poor poor poor #>  [690] poor poor poor [EQ] good [EQ] poor good poor poor good good poor #>  [703] poor poor good poor poor [EQ] [EQ] poor poor good poor good good #>  [716] good good poor poor poor good poor good poor poor [EQ] poor poor #>  [729] poor poor poor poor [EQ] good good good poor poor poor poor poor #>  [742] good poor good good poor poor good poor [EQ] poor poor poor [EQ] #>  [755] good good poor poor poor good poor good poor good [EQ] poor good #>  [768] [EQ] poor [EQ] good poor good [EQ] poor good poor poor good poor #>  [781] poor good good good poor poor poor poor poor good poor [EQ] poor #>  [794] poor poor good [EQ] poor good poor poor good poor good poor poor #>  [807] poor poor poor poor poor poor poor good poor poor poor poor good #>  [820] poor good good poor poor poor poor poor good [EQ] poor good poor #>  [833] poor poor poor poor poor poor poor poor poor poor poor poor good #>  [846] good good poor poor poor poor poor poor poor poor good [EQ] poor #>  [859] [EQ] poor good poor poor poor poor [EQ] good poor good good poor #>  [872] good poor poor good [EQ] poor [EQ] poor poor poor poor [EQ] good #>  [885] poor good poor good poor poor poor poor good poor poor poor poor #>  [898] poor poor poor poor poor poor poor poor good poor good poor poor #>  [911] poor good [EQ] poor good poor poor poor poor good poor poor good #>  [924] good poor poor good poor [EQ] poor good poor good good good poor #>  [937] poor good good poor poor poor poor poor good poor poor good [EQ] #>  [950] poor poor [EQ] poor good [EQ] poor poor good poor poor poor good #>  [963] poor poor poor poor good poor poor [EQ] poor poor poor good good #>  [976] poor poor poor poor poor good poor poor good poor poor good good #>  [989] good good poor [EQ] poor good poor poor poor poor good good good #> [1002] good good poor good [EQ] good poor poor good #> Levels: good poor #> Reportable: 89.8%  # These functions are useful alongside dplyr::mutate() segment_logistic %>%   mutate(     .class_pred = make_two_class_pred(       estimate = .pred_good,       levels = levels(Class),       buffer = 0.15     )   ) #> # A tibble: 1,010 × 4 #>    .pred_poor .pred_good Class .class_pred #>         <dbl>      <dbl> <fct>  <clss_prd> #>  1    0.986      0.0142  poor         poor #>  2    0.897      0.103   poor         poor #>  3    0.118      0.882   good         good #>  4    0.102      0.898   good         good #>  5    0.991      0.00914 poor         poor #>  6    0.633      0.367   good         [EQ] #>  7    0.770      0.230   good         poor #>  8    0.00842    0.992   good         good #>  9    0.995      0.00458 poor         poor #> 10    0.765      0.235   poor         poor #> # ℹ 1,000 more rows  # Multi-class example # Note that we provide class probability columns in the same # order as the levels species_probs %>%   mutate(     .class_pred = make_class_pred(       .pred_bobcat, .pred_coyote, .pred_gray_fox,       levels = levels(Species),       min_prob = .5     )   ) #> # A tibble: 110 × 5 #>    Species  .pred_bobcat .pred_coyote .pred_gray_fox .class_pred #>    <fct>           <dbl>        <dbl>          <dbl>  <clss_prd> #>  1 gray_fox       0.0976       0.0530         0.849     gray_fox #>  2 gray_fox       0.155        0.139          0.706     gray_fox #>  3 bobcat         0.501        0.0880         0.411       bobcat #>  4 gray_fox       0.256        0              0.744     gray_fox #>  5 gray_fox       0.463        0.287          0.250         [EQ] #>  6 bobcat         0.811        0              0.189       bobcat #>  7 bobcat         0.911        0.0888         0           bobcat #>  8 bobcat         0.898        0.0517         0.0500      bobcat #>  9 bobcat         0.771        0.229          0           bobcat #> 10 bobcat         0.623        0.325          0.0517      bobcat #> # ℹ 100 more rows"},{"path":"https://probably.tidymodels.org/dev/reference/predict.int_conformal_infer.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction intervals from conformal methods — predict.int_conformal_infer","title":"Prediction intervals from conformal methods — predict.int_conformal_infer","text":"Prediction intervals conformal methods","code":""},{"path":"https://probably.tidymodels.org/dev/reference/predict.int_conformal_infer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction intervals from conformal methods — predict.int_conformal_infer","text":"","code":"# S3 method for int_conformal_infer predict(object, new_data, level = 0.95, ...)  # S3 method for int_conformal_infer_cv predict(object, new_data, level = 0.95, ...)"},{"path":"https://probably.tidymodels.org/dev/reference/predict.int_conformal_infer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction intervals from conformal methods — predict.int_conformal_infer","text":"object object produced predict.int_conformal_infer(). new_data data frame predictors. level confidence level intervals. ... currently used.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/predict.int_conformal_infer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction intervals from conformal methods — predict.int_conformal_infer","text":"tibble columns .pred_lower .pred_upper. computations prediction bound fail, missing value used.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/probably-package.html","id":null,"dir":"Reference","previous_headings":"","what":"probably: Tools for Post-Processing Class Probability Estimates — probably-package","title":"probably: Tools for Post-Processing Class Probability Estimates — probably-package","text":"Models can improved post-processing class probabilities, : recalibration, conversion hard probabilities, assessment equivocal zones, activities. 'probably' contains tools conducting operations well calibration tools conformal inference techniques regression models.","code":""},{"path":[]},{"path":"https://probably.tidymodels.org/dev/reference/probably-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"probably: Tools for Post-Processing Class Probability Estimates — probably-package","text":"Maintainer: Max Kuhn max@posit.co Authors: Davis Vaughan davis@posit.co Edgar Ruiz edgar@posit.co contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://probably.tidymodels.org/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics .factor, .ordered, augment, fit, required_pkgs","code":""},{"path":"https://probably.tidymodels.org/dev/reference/reportable_rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the reportable rate — reportable_rate","title":"Calculate the reportable rate — reportable_rate","text":"reportable rate defined percentage class predictions equivocal.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/reportable_rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the reportable rate — reportable_rate","text":"","code":"reportable_rate(x)"},{"path":"https://probably.tidymodels.org/dev/reference/reportable_rate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the reportable rate — reportable_rate","text":"x class_pred object.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/reportable_rate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the reportable rate — reportable_rate","text":"reportable rate calculated (n_not_equivocal / n).","code":""},{"path":"https://probably.tidymodels.org/dev/reference/reportable_rate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the reportable rate — reportable_rate","text":"","code":"x <- class_pred(factor(1:5), which = c(1, 2))  # 3 / 5 reportable_rate(x) #> [1] 0.6"},{"path":"https://probably.tidymodels.org/dev/reference/required_pkgs.cal_object.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods to track which additional packages are needed for specific\ncalibrations — required_pkgs.cal_estimate_beta","title":"S3 methods to track which additional packages are needed for specific\ncalibrations — required_pkgs.cal_estimate_beta","text":"S3 methods track additional packages needed specific calibrations","code":""},{"path":"https://probably.tidymodels.org/dev/reference/required_pkgs.cal_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods to track which additional packages are needed for specific\ncalibrations — required_pkgs.cal_estimate_beta","text":"","code":"# S3 method for cal_estimate_beta required_pkgs(x, ...)  # S3 method for cal_estimate_linear_spline required_pkgs(x, ...)  # S3 method for cal_estimate_logistic_spline required_pkgs(x, ...)  # S3 method for cal_estimate_multinomial required_pkgs(x, ...)  # S3 method for cal_object required_pkgs(x, ...)"},{"path":"https://probably.tidymodels.org/dev/reference/required_pkgs.cal_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods to track which additional packages are needed for specific\ncalibrations — required_pkgs.cal_estimate_beta","text":"x calibration object ... arguments passed methods","code":""},{"path":"https://probably.tidymodels.org/dev/reference/segment_naive_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Image segmentation predictions — segment_naive_bayes","title":"Image segmentation predictions — segment_naive_bayes","text":"Image segmentation predictions","code":""},{"path":"https://probably.tidymodels.org/dev/reference/segment_naive_bayes.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Image segmentation predictions — segment_naive_bayes","text":"Hill, LaPan, Li Haney (2007). Impact image segmentation high-content screening data quality SK-BR-3 cells, BMC Bioinformatics, Vol. 8, pg. 340, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/segment_naive_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Image segmentation predictions — segment_naive_bayes","text":"segment_naive_bayes,segment_logistic tibble","code":""},{"path":"https://probably.tidymodels.org/dev/reference/segment_naive_bayes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Image segmentation predictions — segment_naive_bayes","text":"objects contain test set predictions cell segmentation data Hill, LaPan, Li Haney (2007). data frame results different models (naive Bayes logistic regression).","code":""},{"path":"https://probably.tidymodels.org/dev/reference/segment_naive_bayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Image segmentation predictions — segment_naive_bayes","text":"","code":"data(segment_naive_bayes) data(segment_logistic)"},{"path":"https://probably.tidymodels.org/dev/reference/species_probs.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions on animal species — species_probs","title":"Predictions on animal species — species_probs","text":"Predictions animal species","code":""},{"path":"https://probably.tidymodels.org/dev/reference/species_probs.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Predictions on animal species — species_probs","text":"Reid, R. E. B. (2015). morphometric modeling approach distinguishing among bobcat, coyote gray fox scats. Wildlife Biology, 21(5), 254-262","code":""},{"path":"https://probably.tidymodels.org/dev/reference/species_probs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictions on animal species — species_probs","text":"species_probs tibble","code":""},{"path":"https://probably.tidymodels.org/dev/reference/species_probs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predictions on animal species — species_probs","text":"data holdout predictions resampling animal scat data Reid (2015) based C5.0 classification model.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/species_probs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predictions on animal species — species_probs","text":"","code":"data(species_probs) str(species_probs) #> tibble [110 × 4] (S3: tbl_df/tbl/data.frame) #>  $ Species       : Factor w/ 3 levels \"bobcat\",\"coyote\",..: 3 3 1 3 3 1 1 1 1 1 ... #>  $ .pred_bobcat  : num [1:110] 0.0976 0.1548 0.5007 0.2563 0.4627 ... #>  $ .pred_coyote  : num [1:110] 0.053 0.139 0.088 0 0.287 ... #>  $ .pred_gray_fox: num [1:110] 0.849 0.706 0.411 0.744 0.25 ..."},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate performance metrics across probability thresholds — threshold_perf","title":"Generate performance metrics across probability thresholds — threshold_perf","text":"threshold_perf() can take set class probability predictions determine performance characteristics across different values probability threshold existing groups.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate performance metrics across probability thresholds — threshold_perf","text":"","code":"threshold_perf(.data, ...)  # S3 method for data.frame threshold_perf(   .data,   truth,   estimate,   thresholds = NULL,   metrics = NULL,   na_rm = TRUE,   event_level = \"first\",   ... )"},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate performance metrics across probability thresholds — threshold_perf","text":".data tibble, potentially grouped. ... Currently unused. truth column identifier true two-class results (factor). unquoted column name. estimate column identifier predicted class probabilities (numeric). unquoted column name. thresholds numeric vector values probability threshold. unspecified, series values 0.5 1.0 used. Note: argument used, must named. metrics Either NULL yardstick::metric_set() list performance metrics calculate. metrics oriented towards hard class predictions (e.g. yardstick::sensitivity(), yardstick::accuracy(), yardstick::recall(), etc.) class probabilities. set default metrics used NULL (see Details ). na_rm single logical: missing data removed? event_level single string. Either \"first\" \"second\" specify level truth consider \"event\".","code":""},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate performance metrics across probability thresholds — threshold_perf","text":"tibble columns: .threshold, .estimator, .metric, .estimate existing groups.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate performance metrics across probability thresholds — threshold_perf","text":"Note global option yardstick.event_first used determine level event interest. details, see Relevant level section yardstick::sens(). default calculated metrics : yardstick::j_index() yardstick::sens() yardstick::spec() distance = (1 - sens) ^ 2 + (1 - spec) ^ 2 custom metric passed compute sensitivity specificity, distance metric computed.","code":""},{"path":"https://probably.tidymodels.org/dev/reference/threshold_perf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate performance metrics across probability thresholds — threshold_perf","text":"","code":"library(dplyr) data(\"segment_logistic\")  # Set the threshold to 0.6 # > 0.6 = good # < 0.6 = poor threshold_perf(segment_logistic, Class, .pred_good, thresholds = 0.6) #> # A tibble: 3 × 4 #>   .threshold .metric     .estimator .estimate #>        <dbl> <chr>       <chr>          <dbl> #> 1        0.6 sensitivity binary         0.639 #> 2        0.6 specificity binary         0.869 #> 3        0.6 j_index     binary         0.508  # Set the threshold to multiple values thresholds <- seq(0.5, 0.9, by = 0.1)  segment_logistic %>%   threshold_perf(Class, .pred_good, thresholds) #> # A tibble: 15 × 4 #>    .threshold .metric     .estimator .estimate #>         <dbl> <chr>       <chr>          <dbl> #>  1        0.5 sensitivity binary         0.714 #>  2        0.6 sensitivity binary         0.639 #>  3        0.7 sensitivity binary         0.561 #>  4        0.8 sensitivity binary         0.451 #>  5        0.9 sensitivity binary         0.249 #>  6        0.5 specificity binary         0.825 #>  7        0.6 specificity binary         0.869 #>  8        0.7 specificity binary         0.911 #>  9        0.8 specificity binary         0.937 #> 10        0.9 specificity binary         0.977 #> 11        0.5 j_index     binary         0.539 #> 12        0.6 j_index     binary         0.508 #> 13        0.7 j_index     binary         0.472 #> 14        0.8 j_index     binary         0.388 #> 15        0.9 j_index     binary         0.226  # ---------------------------------------------------------------------------  # It works with grouped data frames as well # Let's mock some resampled data resamples <- 5  mock_resamples <- resamples %>%   replicate(     expr = sample_n(segment_logistic, 100, replace = TRUE),     simplify = FALSE   ) %>%   bind_rows(.id = \"resample\")  resampled_threshold_perf <- mock_resamples %>%   group_by(resample) %>%   threshold_perf(Class, .pred_good, thresholds)  resampled_threshold_perf #> # A tibble: 75 × 5 #>    resample .threshold .metric     .estimator .estimate #>    <chr>         <dbl> <chr>       <chr>          <dbl> #>  1 1               0.5 sensitivity binary         0.645 #>  2 1               0.6 sensitivity binary         0.581 #>  3 1               0.7 sensitivity binary         0.516 #>  4 1               0.8 sensitivity binary         0.387 #>  5 1               0.9 sensitivity binary         0.161 #>  6 2               0.5 sensitivity binary         0.606 #>  7 2               0.6 sensitivity binary         0.515 #>  8 2               0.7 sensitivity binary         0.424 #>  9 2               0.8 sensitivity binary         0.333 #> 10 2               0.9 sensitivity binary         0.152 #> # ℹ 65 more rows  # Average over the resamples resampled_threshold_perf %>%   group_by(.metric, .threshold) %>%   summarise(.estimate = mean(.estimate)) #> `summarise()` has grouped output by '.metric'. You can override using the #> `.groups` argument. #> # A tibble: 15 × 3 #> # Groups:   .metric [3] #>    .metric     .threshold .estimate #>    <chr>            <dbl>     <dbl> #>  1 j_index            0.5     0.525 #>  2 j_index            0.6     0.511 #>  3 j_index            0.7     0.443 #>  4 j_index            0.8     0.379 #>  5 j_index            0.9     0.232 #>  6 sensitivity        0.5     0.694 #>  7 sensitivity        0.6     0.634 #>  8 sensitivity        0.7     0.538 #>  9 sensitivity        0.8     0.442 #> 10 sensitivity        0.9     0.250 #> 11 specificity        0.5     0.831 #> 12 specificity        0.6     0.877 #> 13 specificity        0.7     0.904 #> 14 specificity        0.8     0.937 #> 15 specificity        0.9     0.982"},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-development-version","dir":"Changelog","previous_headings":"","what":"probably (development version)","title":"probably (development version)","text":"Copyright holder changed Posit Software PBC. set calibration tools added: need calibration can visualized using collection cal_plot_*() functions. Calibration methods can estimated family cal_estimate_*() functions. validate calibrations using resampling, see cal_validate_*() functions. cal_apply() can take calibration model apply set existing predictions. Possible calibration tools: Binary classification methods: logistic regression, isotonic regression, Beta calibration. Multiclass classification: multinomial, isotonic regression, Beta calibration Regression: linear regression, isotonic regression Based initial PR (#37) Antonio R. Vargas, threshold_perf() now accepts custom metric set (#25) Two functions added compute prediction intervals regression models via conformal inference: int_conformal_infer() int_conformal_infer_cv()","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-010","dir":"Changelog","previous_headings":"","what":"probably 0.1.0","title":"probably 0.1.0","text":"CRAN release: 2022-08-29 Max Kuhn now maintainer (#49). Re-licensed package GPL-2 MIT. copyright holders RStudio employees give consent. Fixed bug make_class_pred() make_two_class_pred() validate levels argument (#42). threshold_perf() now explicit event_level argument rather respecting now deprecated yardstick.event_first global option (#45). Bumped minimum required R version >=3.4.0 align rest tidyverse. Updated testthat 3e (#44).","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-006","dir":"Changelog","previous_headings":"","what":"probably 0.0.6","title":"probably 0.0.6","text":"CRAN release: 2020-06-05 class_pred objects now comparable ordered levels. Equivocal values generally considered smallest value ordering. NA values can considered smaller vec_order(na_value = \"smallest\") used.","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-005","dir":"Changelog","previous_headings":"","what":"probably 0.0.5","title":"probably 0.0.5","text":"CRAN release: 2020-05-14 Internal cleanup compatible vctrs 0.3.0.","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-004","dir":"Changelog","previous_headings":"","what":"probably 0.0.4","title":"probably 0.0.4","text":"CRAN release: 2020-01-13 Suggest modeldata package, lending_club dataset moved removed recipes. Use testthat::verify_output() test expecting specific vctrs error avoid failure CRAN error changes future.","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-003","dir":"Changelog","previous_headings":"","what":"probably 0.0.3","title":"probably 0.0.3","text":"CRAN release: 2019-07-07 probably brought date vctrs 0.2.0. vctrs update many function name changes, required internal refactoring, minimal external changes. one user facing change comes casting one class_pred object another class_pred, factor. previously warning thrown x levels exist , error now generated. consistent vctrs behavior converting one factor another.","code":"x  <- class_pred(factor(\"a\")) to <- class_pred(factor(\"to\")) vec_cast(x, to) #> Error: Lossy cast from <class_pred> to <class_pred>. #> Locations: 1"},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-002","dir":"Changelog","previous_headings":"","what":"probably 0.0.2","title":"probably 0.0.2","text":"CRAN release: 2019-03-07","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"bug-fixes-0-0-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"probably 0.0.2","text":"failing test relying R 3.6 change sample() corrected. rlang warning threshold_perf() fixed. small R 3.1 issue vctrs fixed.","code":""},{"path":"https://probably.tidymodels.org/dev/news/index.html","id":"probably-001","dir":"Changelog","previous_headings":"","what":"probably 0.0.1","title":"probably 0.0.1","text":"CRAN release: 2018-12-18 First release","code":""}]
