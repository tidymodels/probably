% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cal-validate.R
\name{cal_validate_none}
\alias{cal_validate_none}
\alias{cal_validate_none.resample_results}
\alias{cal_validate_none.rset}
\alias{cal_validate_none.tune_results}
\title{Measure performance without using calibration}
\usage{
cal_validate_none(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

\method{cal_validate_none}{resample_results}(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

\method{cal_validate_none}{rset}(
  .data,
  truth = NULL,
  estimate = dplyr::starts_with(".pred_"),
  metrics = NULL,
  save_pred = FALSE,
  ...
)

\method{cal_validate_none}{tune_results}(
  .data,
  truth = NULL,
  estimate = NULL,
  metrics = NULL,
  save_pred = FALSE,
  ...
)
}
\arguments{
\item{.data}{An \code{rset} object or the results of \code{\link[tune:fit_resamples]{tune::fit_resamples()}} with
a \code{.predictions} column.}

\item{truth}{The column identifier for the true class results
(that is a factor). This should be an unquoted column name.}

\item{estimate}{A vector of column identifiers, or one of \code{dplyr} selector
functions to choose which variables contains the class probabilities. It
defaults to the prefix used by tidymodels (\code{.pred_}). The order of the
identifiers will be considered the same as the order of the levels of the
\code{truth} variable.}

\item{metrics}{A set of metrics passed created via \code{\link[yardstick:metric_set]{yardstick::metric_set()}}}

\item{save_pred}{Indicates whether to a column of post-calibration predictions.}

\item{...}{Options to pass to \code{\link[=cal_estimate_logistic]{cal_estimate_logistic()}}, such as the \code{smooth}
argument.}
}
\value{
The original object with a \code{.metrics_cal} column and, optionally,
an additional \code{.predictions_cal} column. The class \code{cal_rset} is also added.
}
\description{
This function uses resampling to measure the effect of calibrating predicted
values.
}
\details{
This function exists to have a complete API for all calibration methods. It
returns the same results "with and without calibration" which, in this case,
is always without calibration.

There are two ways to pass the data in:
\itemize{
\item If you have a data frame of predictions, an \code{rset} object can be created
via \pkg{rsample} functions. See the example below.
\item If you have already made a resampling object from the original data and
used it with \code{\link[tune:fit_resamples]{tune::fit_resamples()}}, you can pass that object to the
calibration function and it will use the same resampling scheme. If a
different resampling scheme should be used, run
\code{\link[tune:collect_predictions]{tune::collect_predictions()}} on the object and use the process in the
previous bullet point.
}

Please note that these functions do not apply to \code{tune_result} objects. The
notion of "validation" implies that the tuning parameter selection has been
resolved.

\code{collect_predictions()} can be used to aggregate the metrics for analysis.
}
\section{Performance Metrics}{


By default, the average of the Brier scores is returned. Any appropriate
\code{\link[yardstick:metric_set]{yardstick::metric_set()}} can be used. The validation function compares the
average of the metrics before, and after the calibration.
}

\examples{

library(dplyr)

species_probs |>
  rsample::vfold_cv() |>
  cal_validate_none(Species) |>
  collect_metrics()

}
\seealso{
\code{\link[=cal_apply]{cal_apply()}}, \code{\link[=cal_estimate_none]{cal_estimate_none()}}
}
